{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a92bb1c1",
   "metadata": {},
   "source": [
    "# Train DistilBERT (multilingual) and convert to TFLite\n",
    "\n",
    "This notebook trains a small multilingual model for phishing detection using `distilbert-base-multilingual-cased`, saves the best checkpoint, converts it to a TensorFlow SavedModel and then to a TFLite file. It includes small tests and saves the artifacts to Google Drive for easy download."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8046c47a",
   "metadata": {},
   "source": [
    "## 0) Notes before you start\n",
    "\n",
    "- Use GPU runtime (Runtime → Change runtime type → GPU).\n",
    "- Upload `train.csv`, `validation.csv`, and `test.csv` via the file upload UI or mount your Drive and place them in a folder.\n",
    "- This notebook is intentionally minimal and uses small defaults to keep runtime short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e273af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Install dependencies (run once)\n",
    "!pip install -q transformers datasets accelerate evaluate sentencepiece\n",
    "!pip install -q 'tensorflow>=2.12'  # for TFLite conversion and interpreter\n",
    "\n",
    "print('Installed packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d23b120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Mount Google Drive (optional) or upload files manually\n",
    "from google.colab import drive, files\n",
    "import os\n",
    "\n",
    "drive_mount_path = '/content/drive'\n",
    "print('If you want to use Drive, run: drive.mount(drive_mount_path) and place CSVs under a folder; otherwise use files.upload()')\n",
    "# Uncomment to mount\n",
    "# drive.mount(drive_mount_path)\n",
    "\n",
    "# Helper: if local files not present, prompt manual upload\n",
    "def ensure_file(path):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Upload {os.path.basename(path)}\")\n",
    "        uploaded = files.upload()\n",
    "        for name in uploaded.keys():\n",
    "            print('Uploaded', name)\n",
    "\n",
    "# Set expected filenames (change if your filenames differ)\n",
    "TRAIN_CSV = 'train.csv'\n",
    "VAL_CSV = 'validation.csv'\n",
    "TEST_CSV = 'test.csv'\n",
    "\n",
    "for p in (TRAIN_CSV, VAL_CSV, TEST_CSV):\n",
    "    ensure_file(p)\n",
    "\n",
    "print('Ready to load CSVs from this runtime workspace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b431bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Load and quick-validate the CSVs\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "val_df = pd.read_csv(VAL_CSV)\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "print('Train', len(train_df), 'Val', len(val_df), 'Test', len(test_df))\n",
    "print('Sample train rows:')\n",
    "display(train_df.head())\n",
    "\n",
    "# Ensure label column exists and map to integers (0 = legitimate, 1 = phishing)\n",
    "label_map = { 'phishing': 1, 'legitimate': 0 }\n",
    "if train_df['label'].dtype != 'int64':\n",
    "    train_df['label'] = train_df['label'].map(label_map).astype('int64')\n",
    "    val_df['label'] = val_df['label'].map(label_map).astype('int64')\n",
    "    test_df['label'] = test_df['label'].map(label_map).astype('int64')\n",
    "\n",
    "print('Label distribution (train):')\n",
    "print(train_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51da0a9f",
   "metadata": {},
   "source": [
    "## 4) Prepare Hugging Face datasets and tokenizer\n",
    "We tokenize with `distilbert-base-multilingual-cased`. We use short sequences (max_length=128) for mobile efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5983c779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = 'distilbert-base-multilingual-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df[['message','label']].rename(columns={'message':'text'}))\n",
    "val_ds = Dataset.from_pandas(val_df[['message','label']].rename(columns={'message':'text'}))\n",
    "test_ds = Dataset.from_pandas(test_df[['message','label']].rename(columns={'message':'text'}))\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "train_ds = train_ds.map(tokenize_fn, batched=True)\n",
    "val_ds = val_ds.map(tokenize_fn, batched=True)\n",
    "test_ds = test_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "train_ds = train_ds.remove_columns(['text']).with_format('torch')\n",
    "val_ds = val_ds.remove_columns(['text']).with_format('torch')\n",
    "test_ds = test_ds.remove_columns(['text']).with_format('torch')\n",
    "\n",
    "print('Datasets tokenized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5ddfcf",
   "metadata": {},
   "source": [
    "## 5) Initialize model and Trainer\n",
    "We use the Hugging Face `Trainer` for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ff665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print('Trainer ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b25128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Train (this may take ~30-60 minutes on Colab GPU depending on dataset size)\n",
    "train_result = trainer.train()\n",
    "print('Training finished')\n",
    "trainer.save_model('./phishing_detector_model')\n",
    "tokenizer.save_pretrained('./phishing_detector_model')\n",
    "print('Model and tokenizer saved to ./phishing_detector_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2c86e3",
   "metadata": {},
   "source": [
    "## 7) Convert PyTorch model to TensorFlow and then to TFLite\n",
    "We convert the saved PyTorch checkpoint to a TF SavedModel using `TFAutoModelForSequenceClassification.from_pretrained(..., from_pt=True)`, then use the TensorFlow Lite converter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117af304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Convert to TensorFlow SavedModel\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "tf_model_dir = './tf_saved_model'\n",
    "if os.path.exists(tf_model_dir):\n",
    "    print('Removing previous TF model')\n",
    "    import shutil\n",
    "    shutil.rmtree(tf_model_dir)\n",
    "\n",
    "print('Loading PyTorch checkpoint and converting to TF...')\n",
    "tf_model = TFAutoModelForSequenceClassification.from_pretrained('./phishing_detector_model', from_pt=True)\n",
    "tf_model.save_pretrained(tf_model_dir)\n",
    "print('Saved TF model to', tf_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a7f641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 Convert SavedModel to TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_dir)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# Use float16 quant if you want a smaller model and your device supports it\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_model = converter.convert()\n",
    "open('phishing_detector.tflite', 'wb').write(tflite_model)\n",
    "print('Wrote phishing_detector.tflite (size MB):', round(len(tflite_model)/(1024*1024),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aea77cd",
   "metadata": {},
   "source": [
    "### 7.3 Prepare representative calibration set (for full-int8 quantization)\n",
    "Use a sample of real messages from the training CSV as the representative dataset. This cell builds `calibration_texts` by sampling up to 500 messages from `TRAIN_CSV` used earlier in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2993d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a representative calibration list from the training CSV (up to 500 samples)\n",
    "import pandas as pd\n",
    "# TRAIN_CSV was defined earlier in the notebook as 'train.csv' — if you mounted Drive or uploaded files, that variable will be present\n",
    "try:\n",
    "    df = pd.read_csv(TRAIN_CSV)\n",
    "except Exception:\n",
    "    # fallback: try the processed path if present in runtime workspace\n",
    "    df = pd.read_csv('data/processed/train.csv')\n",
    "# Ensure we have a message column and drop NA\n",
    "df = df.dropna(subset=['message'])\n",
    "n = min(500, len(df))\n",
    "# stratified-ish sample: sample equally across labels if possible\n",
    "if 'label' in df.columns:\n",
    "    # convert label to string in case it's numeric\n",
    "    df['label'] = df['label'].astype(str)\n",
    "    # group and sample from each label proportionally\n",
    "    groups = []\n",
    "    for _, g in df.groupby('label'):\n",
    "        groups.append(g.sample(frac=min(1, n/len(df)), random_state=42))\n",
    "    sample_df = pd.concat(groups).sample(n=n, random_state=42) if len(df) > n else df.sample(n=n, random_state=42)\n",
    "else:\n",
    "    sample_df = df.sample(n=n, random_state=42)\n",
    "calibration_texts = sample_df['message'].astype(str).tolist()\n",
    "print('Prepared calibration_texts from', len(calibration_texts), 'messages')\n",
    "# show first few examples\n",
    "for t in calibration_texts[:8]:\n",
    "    print('-', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b20ba3c",
   "metadata": {},
   "source": [
    "### 7.4 Full integer (int8) TFLite conversion using the representative dataset\n",
    "This cell runs full integer quantization with the calibration texts built above and writes `phishing_detector_int8.tflite`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f33ac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.4 Full integer (int8) TFLite conversion — robust representative generator\n",
    "import numpy as np\n",
    "import traceback\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer saved earlier during training\n",
    "tokenizer = AutoTokenizer.from_pretrained('./phishing_detector_model')\n",
    "MAX_LEN = 128\n",
    "\n",
    "print('TensorFlow version:', tf.__version__)\n",
    "print('SavedModel path:', tf_model_dir)\n",
    "\n",
    "# Inspect saved model signature (helps confirm input names expected by converter)\n",
    "try:\n",
    "    loaded = tf.saved_model.load(tf_model_dir)\n",
    "    sigs = list(loaded.signatures.keys())\n",
    "    print('SavedModel signatures:', sigs)\n",
    "    if 'serving_default' in sigs:\n",
    "        sd = loaded.signatures['serving_default']\n",
    "        try:\n",
    "            print('serving_default structured_input_signature:', sd.structured_input_signature)\n",
    "        except Exception:\n",
    "            pass\n",
    "except Exception as e:\n",
    "    print('Warning: could not inspect SavedModel signatures:', e)\n",
    "\n",
    "# Representative dataset generator that includes both the bare input names and the serving_default names.\n",
    "# This guarantees the calibrator will find a matching key regardless of SavedModel naming.\n",
    "\n",
    "def representative_with_both():\n",
    "    for t in calibration_texts:\n",
    "        enc = tokenizer(t, truncation=True, padding='max_length', max_length=MAX_LEN, return_tensors='np')\n",
    "        input_ids = enc['input_ids'].astype(np.int32)\n",
    "        attention_mask = enc['attention_mask'].astype(np.int32)\n",
    "        # include both naming variants\n",
    "        yield {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'serving_default_input_ids:0': input_ids,\n",
    "            'serving_default_attention_mask:0': attention_mask,\n",
    "        }\n",
    "\n",
    "# Run conversion to full integer (int8) using the representative generator\n",
    "try:\n",
    "    converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_dir)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.representative_dataset = representative_with_both\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.int8\n",
    "    converter.inference_output_type = tf.int8\n",
    "    print('Running int8 conversion (this may take a while)...')\n",
    "    tflite_int8 = converter.convert()\n",
    "    open('phishing_detector_int8.tflite', 'wb').write(tflite_int8)\n",
    "    print('Wrote phishing_detector_int8.tflite (MB):', round(len(tflite_int8)/(1024*1024),2))\n",
    "except Exception as e:\n",
    "    print('Int8 conversion failed:')\n",
    "    traceback.print_exc()\n",
    "    # As a fallback, write errors to a file for inspection\n",
    "    with open('int8_conversion_error.txt', 'w') as fh:\n",
    "        import traceback as _tb\n",
    "        fh.write(_tb.format_exc())\n",
    "    print('Wrote int8_conversion_error.txt with traceback')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb72ddc",
   "metadata": {},
   "source": [
    "### 7.5 Quick smoke-test for the int8 TFLite model\n",
    "This robust test resizes inputs to (1,128) like your working smoke-test and prints probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a62a55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.5 Robust smoke-tests: dynamic-range, float16, and int8 models\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "import pprint\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('./phishing_detector_model')\n",
    "\n",
    "# Helper: robust inference using resize/allocate approach\n",
    "\n",
    "def prepare_arrays_for_interpreter(enc, input_details):\n",
    "    arrays = []\n",
    "    for inp in input_details:\n",
    "        name = inp['name'].lower()\n",
    "        if 'input_ids' in name or ('input' in name and 'id' in name):\n",
    "            arr = enc.get('input_ids')\n",
    "        elif 'attention' in name and 'mask' in name:\n",
    "            arr = enc.get('attention_mask')\n",
    "        elif 'token_type' in name or 'segment' in name:\n",
    "            arr = enc.get('token_type_ids', None)\n",
    "            if arr is None:\n",
    "                arr = np.zeros_like(enc['input_ids'])\n",
    "        else:\n",
    "            arr = enc.get('input_ids')\n",
    "        if arr is None:\n",
    "            raise RuntimeError(f\"Could not find a suitable tensor for interpreter input '{name}'\")\n",
    "        # normalize dtype\n",
    "        expected_dtype = inp['dtype']\n",
    "        if arr.dtype != expected_dtype:\n",
    "            try:\n",
    "                arr = arr.astype(expected_dtype)\n",
    "            except Exception:\n",
    "                arr = arr.astype(np.int32)\n",
    "        arrays.append(arr)\n",
    "    return arrays\n",
    "\n",
    "\n",
    "def safe_resize_and_allocate(interpreter, input_details, arrays):\n",
    "    resized = False\n",
    "    for inp, arr in zip(input_details, arrays):\n",
    "        current_shape = list(inp['shape'])\n",
    "        desired_shape = list(arr.shape)\n",
    "        if current_shape != desired_shape:\n",
    "            interpreter.resize_tensor_input(inp['index'], desired_shape, strict=False)\n",
    "            resized = True\n",
    "            print(f\"Resized input '{inp['name']}' from {current_shape} -> {desired_shape}\")\n",
    "    if resized:\n",
    "        interpreter.allocate_tensors()\n",
    "        new_input_details = interpreter.get_input_details()\n",
    "        new_output_details = interpreter.get_output_details()\n",
    "        print('Re-queried input details (after resize & allocate):')\n",
    "        pprint.pprint(new_input_details)\n",
    "        return new_input_details, new_output_details\n",
    "    else:\n",
    "        try:\n",
    "            interpreter.allocate_tensors()\n",
    "        except Exception:\n",
    "            pass\n",
    "        return input_details, interpreter.get_output_details()\n",
    "\n",
    "\n",
    "def run_smoke(tflite_path, samples):\n",
    "    print('\\nRunning smoke-test for:', tflite_path)\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    print('input_details:', input_details)\n",
    "    print('output_details:', output_details)\n",
    "\n",
    "    for text in samples:\n",
    "        enc = tokenizer(text, truncation=True, padding='max_length', max_length=128, return_tensors='np')\n",
    "        enc = {k: np.asarray(v) for k, v in enc.items()}\n",
    "        arrays = prepare_arrays_for_interpreter(enc, input_details)\n",
    "        new_input_details, new_output_details = safe_resize_and_allocate(interpreter, input_details, arrays)\n",
    "        for inp, arr in zip(new_input_details, arrays):\n",
    "            idx = inp['index']\n",
    "            expected_shape = tuple(inp['shape'])\n",
    "            if tuple(arr.shape) != expected_shape:\n",
    "                try:\n",
    "                    arr = arr.reshape(expected_shape)\n",
    "                except Exception:\n",
    "                    raise ValueError(f\"Final shape mismatch for input {inp['name']}: tensor shape {arr.shape} vs expected {expected_shape}\")\n",
    "            interpreter.set_tensor(idx, arr)\n",
    "        interpreter.invoke()\n",
    "        out = interpreter.get_tensor(new_output_details[0]['index'])\n",
    "        if new_output_details[0]['dtype'] == np.int8:\n",
    "            scale, zero_point = new_output_details[0]['quantization']\n",
    "            out = (out.astype(np.float32) - zero_point) * scale\n",
    "        import scipy.special\n",
    "        if out.ndim == 2 and out.shape[1] >= 2:\n",
    "            probs = scipy.special.softmax(out, axis=-1)[0].tolist()\n",
    "            pred = int(np.argmax(out, axis=-1)[0])\n",
    "        else:\n",
    "            if out.ndim == 2 and out.shape[1] == 1:\n",
    "                score = 1.0 / (1.0 + np.exp(-out[0][0]))\n",
    "                probs = [1 - float(score), float(score)]\n",
    "                pred = int(score > 0.5)\n",
    "            else:\n",
    "                probs = out.flatten().tolist()\n",
    "                pred = int(np.argmax(out, axis=-1)[0]) if out.size > 1 else int(out.flatten()[0] > 0.5)\n",
    "        print('TEXT:', text)\n",
    "        print('pred:', pred, 'probs:', probs)\n",
    "\n",
    "\n",
    "# Prepare sample messages\n",
    "samples = [\n",
    "    \"URGENT: Your account will be suspended. Click http://fake.example to verify\",\n",
    "    \"Hey, let's meet tomorrow for lunch\"\n",
    "]\n",
    "\n",
    "# Test dynamic-range (if exists)\n",
    "if os.path.exists('phishing_detector_dynamic.tflite'):\n",
    "    run_smoke('phishing_detector_dynamic.tflite', samples)\n",
    "else:\n",
    "    print('phishing_detector_dynamic.tflite not found — dynamic-range test skipped')\n",
    "\n",
    "# Test float16\n",
    "if os.path.exists('phishing_detector.tflite'):\n",
    "    run_smoke('phishing_detector.tflite', samples)\n",
    "else:\n",
    "    print('phishing_detector.tflite not found — float16 test skipped')\n",
    "\n",
    "# Test int8\n",
    "if os.path.exists('phishing_detector_int8.tflite'):\n",
    "    run_smoke('phishing_detector_int8.tflite', samples)\n",
    "else:\n",
    "    print('phishing_detector_int8.tflite not found — int8 test skipped')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02f68a9",
   "metadata": {},
   "source": [
    "## 8) Quick TFLite smoke test\n",
    "We run a small test by tokenizing a sample message and running the TFLite interpreter. This is simplified: a production mobile client should implement the same tokenization flow used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50916b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load tokenizer and tflite model\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('./phishing_detector_model')\n",
    "interpreter = tf.lite.Interpreter(model_path='phishing_detector.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "def predict_tflite(text):\n",
    "    enc = tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='np')\n",
    "    # Hugging Face models expect input IDs and attention mask; input names can vary\n",
    "    # Map inputs by name where possible.\n",
    "    for name, arr in enc.items():\n",
    "        if name == 'input_ids':\n",
    "            interpreter.set_tensor(input_details[0]['index'], arr.astype(np.int32))\n",
    "        elif name == 'attention_mask' and len(input_details) > 1:\n",
    "            # if model has a second input for attention mask\n",
    "            interpreter.set_tensor(input_details[1]['index'], arr.astype(np.int32))\n",
    "\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[0]['index'])\n",
    "    probs = tf.nn.softmax(output, axis=-1).numpy()\n",
    "    pred = int(np.argmax(probs, axis=-1)[0])\n",
    "    return pred, probs[0].tolist()\n",
    "\n",
    "# Test\n",
    "samples = [\n",
    "    'URGENT: Your account will be suspended. Click http://fake.example to verify',\n",
    "    'Hey, let us meet tomorrow for lunch',\n",
    "]\n",
    "for s in samples:\n",
    "    pred, probs = predict_tflite(s)\n",
    "    print(s)\n",
    "    print('pred:', pred, 'probs:', probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e109ccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Save artifacts to Drive (optional) and provide download links\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "dst = '/content/drive/MyDrive/phishing_detector_artifacts'\n",
    "import os, shutil\n",
    "os.makedirs(dst, exist_ok=True)\n",
    "shutil.copy('phishing_detector.tflite', dst)\n",
    "shutil.copytree('./phishing_detector_model', os.path.join(dst, 'phishing_detector_model'), dirs_exist_ok=True)\n",
    "print('Copied artifacts to', dst)\n",
    "print('You can download phishing_detector.tflite from your Drive or use the Colab file browser to download directly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be72835",
   "metadata": {},
   "source": [
    "### Done — next steps\n",
    "\n",
    "- Download `phishing_detector.tflite` and `vocab`/tokenizer files and hand to the mobile dev.\n",
    "- If the TFLite model size is too large for the app, re-run conversion with more aggressive quantization (int8, with a small calibration dataset) — I can add those steps.\n",
    "- If you prefer I prepare a Colab notebook file and commit it to the repo, I can create it and add run instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d724671c",
   "metadata": {},
   "source": [
    "### 9) Package artifacts and copy to Google Drive\n",
    "\n",
    "This cell packages the TFLite artifacts (float16/dynamic/int8), the tokenizer folder, runs a small evaluation on a sample of `test.csv` (if present), writes a README with sizes and quick metrics, zips the package, and copies it to Drive under `phishing_detector_artifacts_v2`.\n",
    "\n",
    "Run this cell after you have the `.tflite` files and the tokenizer saved to `./phishing_detector_model` or `./tokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846cc316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packaging & export cell: create a package, run light evaluation, and copy to Drive\n",
    "import os, shutil, json, time, pathlib\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "try:\n",
    "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "    SKLEARN = True\n",
    "except Exception:\n",
    "    SKLEARN = False\n",
    "\n",
    "# Config\n",
    "DRIVE_DST = '/content/drive/MyDrive/phishing_detector_artifacts_v2'  # change if you want a different path\n",
    "LOCAL_PACKAGE = 'phishing_detector_package'\n",
    "EVAL_SAMPLE = 2000  # number of test rows to sample for quick evaluation (set lower if you want faster runs)\n",
    "MAX_LEN = 128\n",
    "\n",
    "# Mount Drive (will prompt if not mounted yet)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "os.makedirs(DRIVE_DST, exist_ok=True)\n",
    "shutil.rmtree(LOCAL_PACKAGE, ignore_errors=True)\n",
    "os.makedirs(LOCAL_PACKAGE, exist_ok=True)\n",
    "\n",
    "# Gather available TFLite artifacts\n",
    "candidates = [\n",
    "    ('float16', 'phishing_detector.tflite'),\n",
    "    ('dynamic', 'phishing_detector_dynamic.tflite'),\n",
    "    ('int8', 'phishing_detector_int8.tflite'),\n",
    "]\n",
    "found = []\n",
    "for qtype, fname in candidates:\n",
    "    if os.path.exists(fname):\n",
    "        size_mb = round(os.path.getsize(fname) / (1024*1024), 2)\n",
    "        shutil.copy(fname, os.path.join(LOCAL_PACKAGE, fname))\n",
    "        found.append({'quant': qtype, 'file': fname, 'size_mb': size_mb})\n",
    "\n",
    "# Copy tokenizer / tokenizer directory\n",
    "tokenizer_src = None\n",
    "for tok_dir in ('tokenizer', 'phishing_detector_model', './phishing_detector_model'):\n",
    "    if os.path.exists(tok_dir) and os.path.isdir(tok_dir):\n",
    "        tokenizer_src = tok_dir\n",
    "        shutil.copytree(tok_dir, os.path.join(LOCAL_PACKAGE, 'tokenizer'), dirs_exist_ok=True)\n",
    "        break\n",
    "if tokenizer_src is None:\n",
    "    print('Warning: tokenizer folder not found; ensure you include tokenizer files when packaging')\n",
    "\n",
    "# Light evaluation helper (works on a sample of test set to keep runtime short)\n",
    "\n",
    "def run_tflite_eval(tflite_path, test_texts, test_labels, max_eval=500):\n",
    "    # Build interpreter and resize inputs to (1, MAX_LEN)\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    # Resize inputs to (1, MAX_LEN)\n",
    "    for inp in input_details:\n",
    "        interpreter.resize_tensor_input(inp['index'], [1, MAX_LEN])\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    # Evaluate on up to max_eval examples\n",
    "    preds = []\n",
    "    for text in test_texts[:max_eval]:\n",
    "        enc = tokenizer(text, truncation=True, padding='max_length', max_length=MAX_LEN, return_tensors='np')\n",
    "        enc = {k: np.asarray(v) for k,v in enc.items()}\n",
    "        # set inputs in interpreter order\n",
    "        for inp in input_details:\n",
    "            name = inp['name'].lower()\n",
    "            if 'input_ids' in name:\n",
    "                arr = enc.get('input_ids')\n",
    "            elif 'attention_mask' in name:\n",
    "                arr = enc.get('attention_mask')\n",
    "            else:\n",
    "                arr = enc.get('input_ids')\n",
    "            # cast if interpreter expects int8\n",
    "            if inp['dtype'] == np.int8:\n",
    "                arr = arr.astype(np.int8)\n",
    "            else:\n",
    "                arr = arr.astype(inp['dtype'])\n",
    "            interpreter.set_tensor(inp['index'], arr)\n",
    "        interpreter.invoke()\n",
    "        out = interpreter.get_tensor(output_details[0]['index'])\n",
    "        # dequantize if int8 output\n",
    "        if output_details[0]['dtype'] == np.int8:\n",
    "            scale, zero_point = output_details[0]['quantization']\n",
    "            out = (out.astype(np.float32) - zero_point) * scale\n",
    "        # compute pred\n",
    "        if out.ndim == 2 and out.shape[1] >= 2:\n",
    "            pred = int(np.argmax(out, axis=-1)[0])\n",
    "        else:\n",
    "            # fallback: sigmoid scenario\n",
    "            s = 1.0/(1.0+np.exp(-out.ravel()[0]))\n",
    "            pred = int(s > 0.5)\n",
    "        preds.append(pred)\n",
    "    # metrics\n",
    "    if len(preds) == 0:\n",
    "        return None\n",
    "    if SKLEARN:\n",
    "        acc = accuracy_score(test_labels[:min(len(test_labels), max_eval)], preds)\n",
    "        p, r, f1, _ = precision_recall_fscore_support(test_labels[:min(len(test_labels), max_eval)], preds, average='binary')\n",
    "        return {'accuracy': float(acc), 'precision': float(p), 'recall': float(r), 'f1': float(f1), 'n': min(len(test_labels), max_eval)}\n",
    "    else:\n",
    "        # simple accuracy fallback\n",
    "        true = np.array(test_labels[:min(len(test_labels), max_eval)])\n",
    "        arrp = np.array(preds)\n",
    "        acc = float((arrp == true).mean())\n",
    "        return {'accuracy': acc, 'n': min(len(test_labels), max_eval)}\n",
    "\n",
    "# Prepare test data (sample) if available\n",
    "tokenizer = AutoTokenizer.from_pretrained('./phishing_detector_model') if tokenizer_src else None\n",
    "test_df = None\n",
    "if os.path.exists('test.csv'):\n",
    "    test_df = pd.read_csv('test.csv')\n",
    "elif os.path.exists('data/processed/test.csv'):\n",
    "    test_df = pd.read_csv('data/processed/test.csv')\n",
    "\n",
    "eval_results = {}\n",
    "if test_df is not None and 'message' in test_df.columns and 'label' in test_df.columns:\n",
    "    # ensure numeric labels 0/1\n",
    "    if test_df['label'].dtype != 'int64' and test_df['label'].dtype != 'int32':\n",
    "        # try map strings to ints\n",
    "        test_df['label'] = test_df['label'].map({'phishing':1, 'legitimate':0}).fillna(test_df['label'])\n",
    "    labels = test_df['label'].astype(int).tolist()\n",
    "    texts = test_df['message'].astype(str).tolist()\n",
    "    # Evaluate each found tflite file (only short sample to keep time reasonable)\n",
    "    for meta in found:\n",
    "        q = meta['quant']\n",
    "        fname = meta['file']\n",
    "        local_path = os.path.join(LOCAL_PACKAGE, fname)\n",
    "        print('\\nEvaluating', fname, 'on a sample of up to', EVAL_SAMPLE, 'rows...')\n",
    "        res = run_tflite_eval(local_path, texts, labels, max_eval=min(EVAL_SAMPLE, len(texts)))\n",
    "        eval_results[fname] = res\n",
    "else:\n",
    "    print('No test.csv found for evaluation; skipping evaluation step')\n",
    "\n",
    "# Create README with metadata and evaluation results\n",
    "readme_path = os.path.join(LOCAL_PACKAGE, 'README.md')\n",
    "now = datetime.utcnow().isoformat() + 'Z'\n",
    "with open(readme_path, 'w') as fh:\n",
    "    fh.write('# Phishing detector artifacts\\n')\n",
    "    fh.write('\\nCreated: {}\\n'.format(now))\n",
    "    fh.write('\\nModel: distilbert-base-multilingual-cased\\n')\n",
    "    fh.write('max_length: {}\\n'.format(MAX_LEN))\n",
    "    fh.write('input_names: serving_default_input_ids:0, serving_default_attention_mask:0\\n')\n",
    "    fh.write('\\nFiles included:\\n')\n",
    "    for m in found:\n",
    "        fh.write('- {file}  ({quant}, {size_mb} MB)\\n'.format(**m))\n",
    "    if tokenizer_src:\n",
    "        fh.write('- tokenizer folder: {}\\n'.format(tokenizer_src))\n",
    "    fh.write('\\nEvaluation results (sample):\\n')\n",
    "    fh.write(json.dumps(eval_results, indent=2))\n",
    "\n",
    "# Save a small CSV of sample predictions (optional) -- here we'll skip for brevity unless user wants it explicitly\n",
    "\n",
    "# Zip package and copy to Drive\n",
    "archive_name = shutil.make_archive(LOCAL_PACKAGE, 'zip', LOCAL_PACKAGE)\n",
    "shutil.copy(archive_name, DRIVE_DST)\n",
    "print('Packaged artifacts ->', archive_name)\n",
    "print('Copied archive to Drive at', DRIVE_DST)\n",
    "print('Contents written to', LOCAL_PACKAGE)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
