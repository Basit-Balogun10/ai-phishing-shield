{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a92bb1c1"
      },
      "source": [
        "# Train DistilBERT (multilingual) and convert to TFLite\n",
        "\n",
        "This notebook trains a small multilingual model for phishing detection using `distilbert-base-multilingual-cased`, saves the best checkpoint, converts it to a TensorFlow SavedModel and then to a TFLite file. It includes small tests and saves the artifacts to Google Drive for easy download."
      ],
      "id": "a92bb1c1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8046c47a"
      },
      "source": [
        "## 0) Notes before you start\n",
        "\n",
        "- Use GPU runtime (Runtime → Change runtime type → GPU).\n",
        "- Upload `train.csv`, `validation.csv`, and `test.csv` via the file upload UI or mount your Drive and place them in a folder.\n",
        "- This notebook is intentionally minimal and uses small defaults to keep runtime short."
      ],
      "id": "8046c47a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95e273af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21df67b1-ba88-41a8-d0d7-953433575915"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalled packages\n"
          ]
        }
      ],
      "source": [
        "# 1) Install dependencies (run once)\n",
        "!pip install -q transformers datasets accelerate evaluate sentencepiece tensorflow-text\n",
        "!pip install -q 'tensorflow>=2.12'  # for TFLite conversion and interpreter\n",
        "\n",
        "print('Installed packages')"
      ],
      "id": "95e273af"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "1d23b120",
        "outputId": "a104a5bf-8c44-4d84-cd83-a47b1ad0c1bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If you want to use Drive, run: drive.mount(drive_mount_path) and place CSVs under a folder; otherwise use files.upload()\n",
            "Upload train.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fac99f29-51e8-4e77-9d26-ed8f7fafbb5c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-fac99f29-51e8-4e77-9d26-ed8f7fafbb5c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving train.csv to train.csv\n",
            "Uploaded train.csv\n",
            "Upload validation.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-facb2e31-cba9-4af2-98ab-042c4707de3b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-facb2e31-cba9-4af2-98ab-042c4707de3b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving validation.csv to validation.csv\n",
            "Uploaded validation.csv\n",
            "Upload test.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2b8f9e13-a1e9-49fd-8ce2-e76cf6b148ea\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2b8f9e13-a1e9-49fd-8ce2-e76cf6b148ea\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test.csv to test.csv\n",
            "Uploaded test.csv\n",
            "Ready to load CSVs from this runtime workspace\n"
          ]
        }
      ],
      "source": [
        "# 2) Mount Google Drive (optional) or upload files manually\n",
        "from google.colab import drive, files\n",
        "import os\n",
        "\n",
        "drive_mount_path = '/content/drive'\n",
        "print('If you want to use Drive, run: drive.mount(drive_mount_path) and place CSVs under a folder; otherwise use files.upload()')\n",
        "# Uncomment to mount\n",
        "# drive.mount(drive_mount_path)\n",
        "\n",
        "# Helper: if local files not present, prompt manual upload\n",
        "def ensure_file(path):\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"Upload {os.path.basename(path)}\")\n",
        "        uploaded = files.upload()\n",
        "        for name in uploaded.keys():\n",
        "            print('Uploaded', name)\n",
        "\n",
        "# Set expected filenames (change if your filenames differ)\n",
        "TRAIN_CSV = 'train.csv'\n",
        "VAL_CSV = 'validation.csv'\n",
        "TEST_CSV = 'test.csv'\n",
        "\n",
        "for p in (TRAIN_CSV, VAL_CSV, TEST_CSV):\n",
        "    ensure_file(p)\n",
        "\n",
        "print('Ready to load CSVs from this runtime workspace')"
      ],
      "id": "1d23b120"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29b431bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "outputId": "e4779a10-11df-43cb-fc0c-880272781003"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train 47682 Val 10218 Test 10218\n",
            "Sample train rows:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        label                                            message language  \\\n",
              "0  legitimate  Ya ok, vikky vl c witin  &lt;#&gt; mins and il...       en   \n",
              "1    phishing  M-Pesa: Tafadhali thibitisha nambari 579607 ku...       sw   \n",
              "2  legitimate  Kann u schauen 4 mich in da lib ich habe Sache...       de   \n",
              "3    phishing  HARAKA: Akaunti yako ya Benki ya Kenya imesima...       sw   \n",
              "4  legitimate                                कुछ भी वह आ रहा है?       hi   \n",
              "\n",
              "                  source channel      sender          received_at  \\\n",
              "0  huggingface_augmented     sms  2723293735  2025-10-13T05:59:57   \n",
              "1    synthetic_generator     sms  2550087085                  NaN   \n",
              "2    kaggle_multilingual     sms  4976310049  2025-07-26T23:27:19   \n",
              "3    synthetic_generator     sms  2540330598                  NaN   \n",
              "4    kaggle_multilingual     sms  9122515726  2024-12-20T05:18:11   \n",
              "\n",
              "            scam_type  \n",
              "0  legitimate_general  \n",
              "1         otp_request  \n",
              "2  legitimate_general  \n",
              "3  account_suspension  \n",
              "4  legitimate_general  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a97d9d20-eebd-45a1-8cb3-e0b138165672\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>message</th>\n",
              "      <th>language</th>\n",
              "      <th>source</th>\n",
              "      <th>channel</th>\n",
              "      <th>sender</th>\n",
              "      <th>received_at</th>\n",
              "      <th>scam_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>legitimate</td>\n",
              "      <td>Ya ok, vikky vl c witin  &amp;lt;#&amp;gt; mins and il...</td>\n",
              "      <td>en</td>\n",
              "      <td>huggingface_augmented</td>\n",
              "      <td>sms</td>\n",
              "      <td>2723293735</td>\n",
              "      <td>2025-10-13T05:59:57</td>\n",
              "      <td>legitimate_general</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>phishing</td>\n",
              "      <td>M-Pesa: Tafadhali thibitisha nambari 579607 ku...</td>\n",
              "      <td>sw</td>\n",
              "      <td>synthetic_generator</td>\n",
              "      <td>sms</td>\n",
              "      <td>2550087085</td>\n",
              "      <td>NaN</td>\n",
              "      <td>otp_request</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>legitimate</td>\n",
              "      <td>Kann u schauen 4 mich in da lib ich habe Sache...</td>\n",
              "      <td>de</td>\n",
              "      <td>kaggle_multilingual</td>\n",
              "      <td>sms</td>\n",
              "      <td>4976310049</td>\n",
              "      <td>2025-07-26T23:27:19</td>\n",
              "      <td>legitimate_general</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>phishing</td>\n",
              "      <td>HARAKA: Akaunti yako ya Benki ya Kenya imesima...</td>\n",
              "      <td>sw</td>\n",
              "      <td>synthetic_generator</td>\n",
              "      <td>sms</td>\n",
              "      <td>2540330598</td>\n",
              "      <td>NaN</td>\n",
              "      <td>account_suspension</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>legitimate</td>\n",
              "      <td>कुछ भी वह आ रहा है?</td>\n",
              "      <td>hi</td>\n",
              "      <td>kaggle_multilingual</td>\n",
              "      <td>sms</td>\n",
              "      <td>9122515726</td>\n",
              "      <td>2024-12-20T05:18:11</td>\n",
              "      <td>legitimate_general</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a97d9d20-eebd-45a1-8cb3-e0b138165672')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a97d9d20-eebd-45a1-8cb3-e0b138165672 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a97d9d20-eebd-45a1-8cb3-e0b138165672');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-363ad9c6-2c8d-4866-a78d-2f7ce66d28e8\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-363ad9c6-2c8d-4866-a78d-2f7ce66d28e8')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-363ad9c6-2c8d-4866-a78d-2f7ce66d28e8 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(train_df['label']\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"phishing\",\n          \"legitimate\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"message\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"M-Pesa: Tafadhali thibitisha nambari 579607 kukamilisha muamala. Kama si wewe, piga 07015292859\",\n          \"\\u0915\\u0941\\u091b \\u092d\\u0940 \\u0935\\u0939 \\u0906 \\u0930\\u0939\\u093e \\u0939\\u0948?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"language\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"sw\",\n          \"hi\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"huggingface_augmented\",\n          \"synthetic_generator\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"channel\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"sms\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sender\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2842738560,\n        \"min\": 2540330598,\n        \"max\": 9122515726,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2550087085\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"received_at\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"2025-10-13T05:59:57\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"scam_type\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"legitimate_general\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label distribution (train):\n",
            "label\n",
            "0    30497\n",
            "1    17185\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# 3) Load and quick-validate the CSVs\n",
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "val_df = pd.read_csv(VAL_CSV)\n",
        "test_df = pd.read_csv(TEST_CSV)\n",
        "\n",
        "print('Train', len(train_df), 'Val', len(val_df), 'Test', len(test_df))\n",
        "print('Sample train rows:')\n",
        "display(train_df.head())\n",
        "\n",
        "# Ensure label column exists and map to integers (0 = legitimate, 1 = phishing)\n",
        "label_map = { 'phishing': 1, 'legitimate': 0 }\n",
        "if train_df['label'].dtype != 'int64':\n",
        "    train_df['label'] = train_df['label'].map(label_map).astype('int64')\n",
        "    val_df['label'] = val_df['label'].map(label_map).astype('int64')\n",
        "    test_df['label'] = test_df['label'].map(label_map).astype('int64')\n",
        "\n",
        "print('Label distribution (train):')\n",
        "print(train_df['label'].value_counts())"
      ],
      "id": "29b431bc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51da0a9f"
      },
      "source": [
        "## 4) Prepare Hugging Face datasets and tokenizer\n",
        "We tokenize with `distilbert-base-multilingual-cased`. We use short sequences (max_length=128) for mobile efficiency."
      ],
      "id": "51da0a9f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5983c779",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383,
          "referenced_widgets": [
            "aa0af7b360e44f16a68de7e7daccc4d8",
            "90a6d3479a2e4a39a275abe841547fd4",
            "d22d7ac8628f49c0a9b99ef152ba740b",
            "6fbef33592b34c9c87fe35902ae70245",
            "8938476418704aed83dac90e1893a523",
            "20450269afa640a784331df1900cd642",
            "9176086f668344c78306735f3089869c",
            "b79dd75faf5f460f929a54c16977e054",
            "d47f780b019e4da481b0c1387a5b3040",
            "19e340f64b1e47f397f5b7a26fc96625",
            "b57c41dc68a741aca7c8369499889d23",
            "e13c6453db5f4cacb5bbc64469de7c69",
            "2a8a1fd70ea14ef88a703532c340a2c9",
            "b557aeeafa224eefbd47d5eca4f353ac",
            "16df824fb8c64fd1abf65b442e695642",
            "0bcccd5f3c8f4b6cb6a82cad2655173d",
            "e6bbbc6b9ee742d28a72dcf6a02a10c4",
            "fdbe4ce1e50046d78c5e9edf7d0c1c96",
            "8d94f3b6c29b408286d0d1e3e261f983",
            "94fc49dc951c4d8e81f257e2bcad4fd7",
            "55b5be88433a43818f0a817772628a05",
            "21bd8fd581cb4bfa9a240ebda672bddd",
            "3429e3d205c240328427dc8c1811da17",
            "27e0ce614e364cedaea412fc227be824",
            "6b93681694d74e09aaf56834d34eb8ab",
            "1c3bc48741064d859a8510d1a0ec0659",
            "d2d498ba73dc4d37a36c828761ec040b",
            "b45d6159f2d247f4a1c1bb9f19e7f91c",
            "4ef3eec9b7194495bad6e52797978da7",
            "161f1d6352124c97bbe4facbfa4e3b45",
            "c94757f85f1f44a8ae7b875ea1182a2f",
            "5920577a66f5421ba40904c3c68d0fa1",
            "d59bfd6f73a44238a391949e25f37e22",
            "ea533343b18f485c9dfd0064e7cc5f8a",
            "63de43b629c34e89b9ae83df7b952fd1",
            "133bc682949d4ca281030aa47ac678a0",
            "7c419b5e0de0440791ef317d8797e32d",
            "4ebbd3183b46427cbe6a6fdb32b64794",
            "55ccee12d05e4a81af09f085a9ace1db",
            "9bc8962228314134ab554babf44e93e2",
            "8524339f2f194943be35f5fb7355d56f",
            "dda05d076ce344a389263290d169ba3f",
            "596dd0991a1b49eea693c61edc28c645",
            "1a4af1c3d4704de4a4eff946e15a066f",
            "dff5ec32f9b342d58196c585fcee8206",
            "68dc5dea02e149328e8da6e202ec0a4f",
            "bb4ecddcf89b44d0ba638b13c7cd44ed",
            "661117fb55234731bb3b29fe37da6c22",
            "8865bd5f1f334becbd75daebfc223e73",
            "6d797352fcba412fb84241078cc34dce",
            "040845ca4d094d249b3dc5db9d5def87",
            "037b6739e0f54240b40748d785aa2f33",
            "d2d704679b8449fc90aafe98b5308d55",
            "9369a79aefb140b795bcc09b70c59236",
            "6ef195f30482441b91e51458801f9343",
            "9b3aafb15d544d29a9e09c867a197118",
            "8c0820f474b84ae1bc7cbb865362da8f",
            "f59921a53d0648c9b6afe6ec2ca7d157",
            "f2e9c565566d43b3aaf1a2682760dfdb",
            "b6a0aa0e708e4c928c2b70e0679bc806",
            "5927df43863f44dfbd877b64aa7dc8cb",
            "a0bb9b9fdbc44d23987ec3571151fcbb",
            "0a1a24374a9d44d49e18293d60d06d5f",
            "327644b48ecf4cab8de3f83ce934c60a",
            "a80137bbf39d4ad784f1991741d239d4",
            "9f440099686946639bf8497b8afdb6f2",
            "3a3e13c1f5784f79ae0794c8a1bac9cb",
            "17325d9511004950bb499b61e6818886",
            "becbf656017641ecad7283f997ec8d9c",
            "aba7fa0265c849aabc1ab19439f62e97",
            "db6d4e50057c43ee9f7670f122beb3f2",
            "a57a28ca11784c4c99bcf664d9f2847e",
            "136d6254f8cd4bc1b3f9063a0ea6bdce",
            "ced662a561c44de882b59294ee4ec39a",
            "6277637cb3fb4d0c82e2d76421edb267",
            "8c4c75585f394df3a751875db9ccdc3e",
            "68ac09fdf76e4b0abdcdf4dfddcf9103"
          ]
        },
        "outputId": "1aac5cfe-8dc5-478d-edcd-f1ced077f6dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa0af7b360e44f16a68de7e7daccc4d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e13c6453db5f4cacb5bbc64469de7c69"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3429e3d205c240328427dc8c1811da17"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea533343b18f485c9dfd0064e7cc5f8a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/47682 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dff5ec32f9b342d58196c585fcee8206"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/10218 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b3aafb15d544d29a9e09c867a197118"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/10218 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a3e13c1f5784f79ae0794c8a1bac9cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets tokenized\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = 'distilbert-base-multilingual-cased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "train_ds = Dataset.from_pandas(train_df[['message','label']].rename(columns={'message':'text'}))\n",
        "val_ds = Dataset.from_pandas(val_df[['message','label']].rename(columns={'message':'text'}))\n",
        "test_ds = Dataset.from_pandas(test_df[['message','label']].rename(columns={'message':'text'}))\n",
        "\n",
        "def tokenize_fn(batch):\n",
        "    return tokenizer(batch['text'], padding='max_length', truncation=True, max_length=128)\n",
        "\n",
        "train_ds = train_ds.map(tokenize_fn, batched=True)\n",
        "val_ds = val_ds.map(tokenize_fn, batched=True)\n",
        "test_ds = test_ds.map(tokenize_fn, batched=True)\n",
        "\n",
        "train_ds = train_ds.remove_columns(['text']).with_format('torch')\n",
        "val_ds = val_ds.remove_columns(['text']).with_format('torch')\n",
        "test_ds = test_ds.remove_columns(['text']).with_format('torch')\n",
        "\n",
        "print('Datasets tokenized')"
      ],
      "id": "5983c779"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b5ddfcf"
      },
      "source": [
        "## 5) Initialize model and Trainer\n",
        "We use the Hugging Face `Trainer` for simplicity."
      ],
      "id": "8b5ddfcf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f8ff665",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "f3b0e5d664fb40dda4fdcc0068324f06",
            "c54a22bd22a740ebba984d30f399c79e",
            "fdebe77aee4544568150bfe53fb9252d",
            "9c4aaa2e587c43a89f8bb33ac69c9dc8",
            "795d6f7ca47144fba8d509f4579ab4ce",
            "895543076219492187b9f4a97c8164fb",
            "d7cf0bb0d9264f0699d000c750f4204c",
            "b03a09aa57764a32a2a2d9409f4fecde",
            "2541ccf2065047b6ad3316b77f35961e",
            "4ab6cbb946a4489796677ca9b0f83b91",
            "43eadb315f2443a591374320b63610c6"
          ]
        },
        "outputId": "0de5fac1-d6b5-423a-e56b-c6949e6c809d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/542M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3b0e5d664fb40dda4fdcc0068324f06"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainer ready\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    eval_strategy='epoch', # Corrected from evaluation_strategy\n",
        "    save_strategy='epoch',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='accuracy',\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print('Trainer ready')"
      ],
      "id": "5f8ff665"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9b25128",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "outputId": "693ee05e-e2f4-4bd9-f812-801640d6bb43"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7794' max='8943' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7794/8943 27:17 < 04:01, 4.76 it/s, Epoch 2.61/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.017100</td>\n",
              "      <td>0.042983</td>\n",
              "      <td>0.992269</td>\n",
              "      <td>0.989227</td>\n",
              "      <td>0.993699</td>\n",
              "      <td>0.984795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.011000</td>\n",
              "      <td>0.060179</td>\n",
              "      <td>0.991486</td>\n",
              "      <td>0.988103</td>\n",
              "      <td>0.995317</td>\n",
              "      <td>0.980994</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8944' max='8943' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8943/8943 30:59, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.017100</td>\n",
              "      <td>0.042983</td>\n",
              "      <td>0.992269</td>\n",
              "      <td>0.989227</td>\n",
              "      <td>0.993699</td>\n",
              "      <td>0.984795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.011000</td>\n",
              "      <td>0.060179</td>\n",
              "      <td>0.991486</td>\n",
              "      <td>0.988103</td>\n",
              "      <td>0.995317</td>\n",
              "      <td>0.980994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.005500</td>\n",
              "      <td>0.048656</td>\n",
              "      <td>0.993051</td>\n",
              "      <td>0.990326</td>\n",
              "      <td>0.993982</td>\n",
              "      <td>0.986696</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# 6) Train (this may take ~30-60 minutes on Colab GPU depending on dataset size)\n",
        "train_result = trainer.train()\n",
        "print('Training finished')\n",
        "trainer.save_model('./phishing_detector_model')\n",
        "tokenizer.save_pretrained('./phishing_detector_model')\n",
        "print('Model and tokenizer saved to ./phishing_detector_model')"
      ],
      "id": "e9b25128"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e2c86e3"
      },
      "source": [
        "## 7) Convert PyTorch model to TensorFlow and then to TFLite\n",
        "We convert the saved PyTorch checkpoint to a TF SavedModel using `TFAutoModelForSequenceClassification.from_pretrained(..., from_pt=True)`, then use the TensorFlow Lite converter."
      ],
      "id": "7e2c86e3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "117af304",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1825f392-e19e-4317-e99b-21c68d19ba95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading PyTorch checkpoint and converting to TF...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
            "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
            "\n",
            "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x78b76cbea8d0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x78b76ca5f050>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x78b745b4ddf0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x78b73bdb2de0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x78b73bdb1430>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x78b745ab9040>, because it is not built.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved TF model to ./tf_saved_model\n"
          ]
        }
      ],
      "source": [
        "# 7.1 Convert to TensorFlow SavedModel\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "tf_model_dir = './tf_saved_model'\n",
        "if os.path.exists(tf_model_dir):\n",
        "    print('Removing previous TF model')\n",
        "    import shutil\n",
        "    shutil.rmtree(tf_model_dir)\n",
        "\n",
        "print('Loading PyTorch checkpoint and converting to TF...')\n",
        "tf_model = TFAutoModelForSequenceClassification.from_pretrained('./phishing_detector_model', from_pt=True)\n",
        "tf_model.save(tf_model_dir)\n",
        "print('Saved TF model to', tf_model_dir)"
      ],
      "id": "117af304"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86a7f641",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3ef1709-f205-4c40-e56a-5396f414c3ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Float16 - Wrote phishing_detector.tflite (size MB): 258.27\n",
            "Dynamic-range quanitzation - Wrote phishing_detector_dynamic.tflite (MB): 129.86\n"
          ]
        }
      ],
      "source": [
        "# 7.2 Convert SavedModel to TFLite\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_dir)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# Use float16 quant if you want a smaller model and your device supports it\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "tflite_model = converter.convert()\n",
        "open('phishing_detector.tflite', 'wb').write(tflite_model)\n",
        "print('Float16 - Wrote phishing_detector.tflite (size MB):', round(len(tflite_model)/(1024*1024),2))\n",
        "\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_dir)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_dyn = converter.convert()\n",
        "open('phishing_detector_dynamic.tflite', 'wb').write(tflite_dyn)\n",
        "print('Dynamic-range quanitzation - Wrote phishing_detector_dynamic.tflite (MB):', round(len(tflite_dyn)/(1024*1024), 2))"
      ],
      "id": "86a7f641"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.3 Prepare representative calibration set (for full-int8 quantization)\n",
        "Use a sample of real messages from the training CSV as the representative dataset. This cell builds `calibration_texts` by sampling up to 500 messages from `TRAIN_CSV` used earlier in the notebook."
      ],
      "metadata": {
        "id": "0R0LxpL8GS7v"
      },
      "id": "0R0LxpL8GS7v"
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a representative calibration list from the training CSV (up to 500 samples)\n",
        "import pandas as pd\n",
        "# TRAIN_CSV was defined earlier in the notebook as 'train.csv' — if you mounted Drive or uploaded files, that variable will be present\n",
        "try:\n",
        "    df = pd.read_csv(TRAIN_CSV)\n",
        "except Exception:\n",
        "    # fallback: try the processed path if present in runtime workspace\n",
        "    df = pd.read_csv('data/processed/train.csv')\n",
        "# Ensure we have a message column and drop NA\n",
        "df = df.dropna(subset=['message'])\n",
        "n = min(500, len(df))\n",
        "# stratified-ish sample: sample equally across labels if possible\n",
        "if 'label' in df.columns:\n",
        "    # convert label to string in case it's numeric\n",
        "    df['label'] = df['label'].astype(str)\n",
        "    # group and sample from each label proportionally\n",
        "    groups = []\n",
        "    for _, g in df.groupby('label'):\n",
        "        groups.append(g.sample(frac=min(1, n/len(df)), random_state=42))\n",
        "    sample_df = pd.concat(groups).sample(n=n, random_state=42) if len(df) > n else df.sample(n=n, random_state=42)\n",
        "else:\n",
        "    sample_df = df.sample(n=n, random_state=42)\n",
        "calibration_texts = sample_df['message'].astype(str).tolist()\n",
        "print('Prepared calibration_texts from', len(calibration_texts), 'messages')\n",
        "# show first few examples\n",
        "for t in calibration_texts[:8]:\n",
        "    print('-', t)"
      ],
      "metadata": {
        "id": "rjR_TYwjGXW2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b665c68c-c389-479f-a73f-6a39f486cab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared calibration_texts from 500 messages\n",
            "- FREE camera phones with linerental from 4.49/month with 750 cross ntwk mins. 1/2 price txt bundle deals also avble. Call 08001950382 or call2optout/J MF\n",
            "- كيف أنت؟ فقط التحقق من نفسك\n",
            "- Wir versuchten, Sie zu kontaktieren re Ihre Antwort auf unser Angebot eines Video Handset? 750 jederzeit irgendwelche Netzwerke Minuten? UNLIMITED TEXT? Camcorder? Antworten oder rufen Sie 08000930705 JETZT\n",
            "- You have received ₦5,000 from EKEDC. Balance: ₦5,000. Ref: EPRY0ZAGOF\n",
            "- Ja, ich bin ein Mann mit einer Frau! Bitte sag mir, was du magst und was du im Bett nicht magst.\n",
            "- Nlekọta Ndị Ahịa First Bank: Akaụntụ gị chọrọ mmelite. Kpọọ anyị ozugbo na 08063357276\n",
            "- Hongera! Umepokea KES 20,000 kutoka Benki ya Kenya. Bonyeza https://cutt.ly/c937 kudai pesa zako\n",
            "- Tu as encore de la bière chez toi?\n"
          ]
        }
      ],
      "id": "rjR_TYwjGXW2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4 Full integer (int8) TFLite conversion using the representative dataset\n",
        "This cell runs full integer quantization with the calibration texts built above and writes `phishing_detector_int8.tflite`."
      ],
      "metadata": {
        "id": "rwdnjk_yGZlf"
      },
      "id": "rwdnjk_yGZlf"
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.4 Full integer (int8) TFLite conversion — robust representative generator\n",
        "import numpy as np\n",
        "import traceback\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load tokenizer saved earlier during training\n",
        "tokenizer = AutoTokenizer.from_pretrained('./phishing_detector_model')\n",
        "MAX_LEN = 128\n",
        "\n",
        "print('TensorFlow version:', tf.__version__)\n",
        "print('SavedModel path:', tf_model_dir)\n",
        "\n",
        "# Inspect saved model signature (helps confirm input names expected by converter)\n",
        "try:\n",
        "    loaded = tf.saved_model.load(tf_model_dir)\n",
        "    sigs = list(loaded.signatures.keys())\n",
        "    print('SavedModel signatures:', sigs)\n",
        "    if 'serving_default' in sigs:\n",
        "        sd = loaded.signatures['serving_default']\n",
        "        try:\n",
        "            print('serving_default structured_input_signature:', sd.structured_input_signature)\n",
        "        except Exception:\n",
        "            pass\n",
        "except Exception as e:\n",
        "    print('Warning: could not inspect SavedModel signatures:', e)\n",
        "\n",
        "# Representative dataset generator that includes both the bare input names and the serving_default names.\n",
        "# This guarantees the calibrator will find a matching key regardless of SavedModel naming.\n",
        "\n",
        "def representative_with_both():\n",
        "    for t in calibration_texts:\n",
        "        enc = tokenizer(t, truncation=True, padding='max_length', max_length=MAX_LEN, return_tensors='np')\n",
        "        input_ids = enc['input_ids'].astype(np.int32)\n",
        "        attention_mask = enc['attention_mask'].astype(np.int32)\n",
        "        # include both naming variants\n",
        "        yield {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'serving_default_input_ids:0': input_ids,\n",
        "            'serving_default_attention_mask:0': attention_mask,\n",
        "        }\n",
        "\n",
        "# Run conversion to full integer (int8) using the representative generator\n",
        "try:\n",
        "    converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_dir)\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    converter.representative_dataset = representative_with_both\n",
        "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "    converter.inference_input_type = tf.int8\n",
        "    converter.inference_output_type = tf.int8\n",
        "    print('Running int8 conversion (this may take a while)...')\n",
        "    tflite_int8 = converter.convert()\n",
        "    open('phishing_detector_int8.tflite', 'wb').write(tflite_int8)\n",
        "    print('Wrote phishing_detector_int8.tflite (MB):', round(len(tflite_int8)/(1024*1024),2))\n",
        "except Exception as e:\n",
        "    print('Int8 conversion failed:')\n",
        "    traceback.print_exc()\n",
        "    # As a fallback, write errors to a file for inspection\n",
        "    with open('int8_conversion_error.txt', 'w') as fh:\n",
        "        import traceback as _tb\n",
        "        fh.write(_tb.format_exc())\n",
        "    print('Wrote int8_conversion_error.txt with traceback')\n"
      ],
      "metadata": {
        "id": "7CVpQ-bfGfii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b0c549b-3327-492d-942d-6a905ac7f6aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.19.0\n",
            "SavedModel path: ./tf_saved_model\n",
            "SavedModel signatures: ['serving_default']\n",
            "serving_default structured_input_signature: ((), {'attention_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name='attention_mask'), 'input_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_ids')})\n",
            "Running int8 conversion (this may take a while)...\n",
            "Wrote phishing_detector_int8.tflite (MB): 129.71\n"
          ]
        }
      ],
      "id": "7CVpQ-bfGfii"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.5 Quick smoke-test for the int8 TFLite model\n",
        "This robust test resizes inputs to (1,128) like your working smoke-test and prints probabilities."
      ],
      "metadata": {
        "id": "ijfsCSaSGj0T"
      },
      "id": "ijfsCSaSGj0T"
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.5 Robust smoke-tests: dynamic-range, float16, and int8 models\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer\n",
        "import pprint\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('./phishing_detector_model')\n",
        "\n",
        "# Helper: robust inference using resize/allocate approach\n",
        "\n",
        "def prepare_arrays_for_interpreter(enc, input_details):\n",
        "    arrays = []\n",
        "    for inp in input_details:\n",
        "        name = inp['name'].lower()\n",
        "        if 'input_ids' in name or ('input' in name and 'id' in name):\n",
        "            arr = enc.get('input_ids')\n",
        "        elif 'attention' in name and 'mask' in name:\n",
        "            arr = enc.get('attention_mask')\n",
        "        elif 'token_type' in name or 'segment' in name:\n",
        "            arr = enc.get('token_type_ids', None)\n",
        "            if arr is None:\n",
        "                arr = np.zeros_like(enc['input_ids'])\n",
        "        else:\n",
        "            arr = enc.get('input_ids')\n",
        "        if arr is None:\n",
        "            raise RuntimeError(f\"Could not find a suitable tensor for interpreter input '{name}'\")\n",
        "        # normalize dtype\n",
        "        expected_dtype = inp['dtype']\n",
        "        if arr.dtype != expected_dtype:\n",
        "            try:\n",
        "                arr = arr.astype(expected_dtype)\n",
        "            except Exception:\n",
        "                arr = arr.astype(np.int32)\n",
        "        arrays.append(arr)\n",
        "    return arrays\n",
        "\n",
        "\n",
        "def safe_resize_and_allocate(interpreter, input_details, arrays):\n",
        "    resized = False\n",
        "    for inp, arr in zip(input_details, arrays):\n",
        "        current_shape = list(inp['shape'])\n",
        "        desired_shape = list(arr.shape)\n",
        "        if current_shape != desired_shape:\n",
        "            interpreter.resize_tensor_input(inp['index'], desired_shape, strict=False)\n",
        "            resized = True\n",
        "            print(f\"Resized input '{inp['name']}' from {current_shape} -> {desired_shape}\")\n",
        "    if resized:\n",
        "        interpreter.allocate_tensors()\n",
        "        new_input_details = interpreter.get_input_details()\n",
        "        new_output_details = interpreter.get_output_details()\n",
        "        print('Re-queried input details (after resize & allocate):')\n",
        "        pprint.pprint(new_input_details)\n",
        "        return new_input_details, new_output_details\n",
        "    else:\n",
        "        try:\n",
        "            interpreter.allocate_tensors()\n",
        "        except Exception:\n",
        "            pass\n",
        "        return input_details, interpreter.get_output_details()\n",
        "\n",
        "\n",
        "def run_smoke(tflite_path, samples):\n",
        "    print('\\nRunning smoke-test for:', tflite_path)\n",
        "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    print('input_details:', input_details)\n",
        "    print('output_details:', output_details)\n",
        "\n",
        "    for text in samples:\n",
        "        enc = tokenizer(text, truncation=True, padding='max_length', max_length=128, return_tensors='np')\n",
        "        enc = {k: np.asarray(v) for k, v in enc.items()}\n",
        "        arrays = prepare_arrays_for_interpreter(enc, input_details)\n",
        "        new_input_details, new_output_details = safe_resize_and_allocate(interpreter, input_details, arrays)\n",
        "        for inp, arr in zip(new_input_details, arrays):\n",
        "            idx = inp['index']\n",
        "            expected_shape = tuple(inp['shape'])\n",
        "            if tuple(arr.shape) != expected_shape:\n",
        "                try:\n",
        "                    arr = arr.reshape(expected_shape)\n",
        "                except Exception:\n",
        "                    raise ValueError(f\"Final shape mismatch for input {inp['name']}: tensor shape {arr.shape} vs expected {expected_shape}\")\n",
        "            interpreter.set_tensor(idx, arr)\n",
        "        interpreter.invoke()\n",
        "        out = interpreter.get_tensor(new_output_details[0]['index'])\n",
        "        if new_output_details[0]['dtype'] == np.int8:\n",
        "            scale, zero_point = new_output_details[0]['quantization']\n",
        "            out = (out.astype(np.float32) - zero_point) * scale\n",
        "        import scipy.special\n",
        "        if out.ndim == 2 and out.shape[1] >= 2:\n",
        "            probs = scipy.special.softmax(out, axis=-1)[0].tolist()\n",
        "            pred = int(np.argmax(out, axis=-1)[0])\n",
        "        else:\n",
        "            if out.ndim == 2 and out.shape[1] == 1:\n",
        "                score = 1.0 / (1.0 + np.exp(-out[0][0]))\n",
        "                probs = [1 - float(score), float(score)]\n",
        "                pred = int(score > 0.5)\n",
        "            else:\n",
        "                probs = out.flatten().tolist()\n",
        "                pred = int(np.argmax(out, axis=-1)[0]) if out.size > 1 else int(out.flatten()[0] > 0.5)\n",
        "        print('TEXT:', text)\n",
        "        print('pred:', pred, 'probs:', probs)\n",
        "\n",
        "\n",
        "# Prepare sample messages\n",
        "samples = [\n",
        "    \"URGENT: Your account will be suspended. Click http://fake.example to verify\",\n",
        "    \"Hey, let's meet tomorrow for lunch\"\n",
        "]\n",
        "\n",
        "# Test dynamic-range (if exists)\n",
        "if os.path.exists('phishing_detector_dynamic.tflite'):\n",
        "    run_smoke('phishing_detector_dynamic.tflite', samples)\n",
        "else:\n",
        "    print('phishing_detector_dynamic.tflite not found — dynamic-range test skipped')\n",
        "\n",
        "# Test float16\n",
        "if os.path.exists('phishing_detector.tflite'):\n",
        "    run_smoke('phishing_detector.tflite', samples)\n",
        "else:\n",
        "    print('phishing_detector.tflite not found — float16 test skipped')\n",
        "\n",
        "# Test int8\n",
        "if os.path.exists('phishing_detector_int8.tflite'):\n",
        "    run_smoke('phishing_detector_int8.tflite', samples)\n",
        "else:\n",
        "    print('phishing_detector_int8.tflite not found — int8 test skipped')\n"
      ],
      "metadata": {
        "id": "WQXymn0rGnmo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eb1bd39-6694-49f3-93d9-886cab3a0825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running smoke-test for: phishing_detector_dynamic.tflite\n",
            "input_details: [{'name': 'serving_default_attention_mask:0', 'index': 0, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([-1, -1], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'serving_default_input_ids:0', 'index': 1, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([-1, -1], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "output_details: [{'name': 'StatefulPartitionedCall:0', 'index': 719, 'shape': array([1, 2], dtype=int32), 'shape_signature': array([-1,  2], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "Resized input 'serving_default_attention_mask:0' from [np.int32(1), np.int32(1)] -> [1, 128]\n",
            "Resized input 'serving_default_input_ids:0' from [np.int32(1), np.int32(1)] -> [1, 128]\n",
            "Re-queried input details (after resize & allocate):\n",
            "[{'dtype': <class 'numpy.int32'>,\n",
            "  'index': 0,\n",
            "  'name': 'serving_default_attention_mask:0',\n",
            "  'quantization': (0.0, 0),\n",
            "  'quantization_parameters': {'quantized_dimension': 0,\n",
            "                              'scales': array([], dtype=float32),\n",
            "                              'zero_points': array([], dtype=int32)},\n",
            "  'shape': array([  1, 128], dtype=int32),\n",
            "  'shape_signature': array([-1, -1], dtype=int32),\n",
            "  'sparsity_parameters': {}},\n",
            " {'dtype': <class 'numpy.int32'>,\n",
            "  'index': 1,\n",
            "  'name': 'serving_default_input_ids:0',\n",
            "  'quantization': (0.0, 0),\n",
            "  'quantization_parameters': {'quantized_dimension': 0,\n",
            "                              'scales': array([], dtype=float32),\n",
            "                              'zero_points': array([], dtype=int32)},\n",
            "  'shape': array([  1, 128], dtype=int32),\n",
            "  'shape_signature': array([-1, -1], dtype=int32),\n",
            "  'sparsity_parameters': {}}]\n",
            "TEXT: URGENT: Your account will be suspended. Click http://fake.example to verify\n",
            "pred: 1 probs: [2.154530920961406e-05, 0.9999784231185913]\n",
            "Resized input 'serving_default_attention_mask:0' from [np.int32(1), np.int32(1)] -> [1, 128]\n",
            "Resized input 'serving_default_input_ids:0' from [np.int32(1), np.int32(1)] -> [1, 128]\n",
            "Re-queried input details (after resize & allocate):\n",
            "[{'dtype': <class 'numpy.int32'>,\n",
            "  'index': 0,\n",
            "  'name': 'serving_default_attention_mask:0',\n",
            "  'quantization': (0.0, 0),\n",
            "  'quantization_parameters': {'quantized_dimension': 0,\n",
            "                              'scales': array([], dtype=float32),\n",
            "                              'zero_points': array([], dtype=int32)},\n",
            "  'shape': array([  1, 128], dtype=int32),\n",
            "  'shape_signature': array([-1, -1], dtype=int32),\n",
            "  'sparsity_parameters': {}},\n",
            " {'dtype': <class 'numpy.int32'>,\n",
            "  'index': 1,\n",
            "  'name': 'serving_default_input_ids:0',\n",
            "  'quantization': (0.0, 0),\n",
            "  'quantization_parameters': {'quantized_dimension': 0,\n",
            "                              'scales': array([], dtype=float32),\n",
            "                              'zero_points': array([], dtype=int32)},\n",
            "  'shape': array([  1, 128], dtype=int32),\n",
            "  'shape_signature': array([-1, -1], dtype=int32),\n",
            "  'sparsity_parameters': {}}]\n",
            "TEXT: Hey, let's meet tomorrow for lunch\n",
            "pred: 0 probs: [0.9999953508377075, 4.658450507122325e-06]\n",
            "\n",
            "Running smoke-test for: phishing_detector.tflite\n",
            "input_details: [{'name': 'serving_default_attention_mask:0', 'index': 0, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([-1, -1], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'serving_default_input_ids:0', 'index': 1, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([-1, -1], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "output_details: [{'name': 'StatefulPartitionedCall:0', 'index': 825, 'shape': array([1, 2], dtype=int32), 'shape_signature': array([-1,  2], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "Resized input 'serving_default_attention_mask:0' from [np.int32(1), np.int32(1)] -> [1, 128]\n",
            "Resized input 'serving_default_input_ids:0' from [np.int32(1), np.int32(1)] -> [1, 128]\n",
            "Re-queried input details (after resize & allocate):\n",
            "[{'dtype': <class 'numpy.int32'>,\n",
            "  'index': 0,\n",
            "  'name': 'serving_default_attention_mask:0',\n",
            "  'quantization': (0.0, 0),\n",
            "  'quantization_parameters': {'quantized_dimension': 0,\n",
            "                              'scales': array([], dtype=float32),\n",
            "                              'zero_points': array([], dtype=int32)},\n",
            "  'shape': array([  1, 128], dtype=int32),\n",
            "  'shape_signature': array([-1, -1], dtype=int32),\n",
            "  'sparsity_parameters': {}},\n",
            " {'dtype': <class 'numpy.int32'>,\n",
            "  'index': 1,\n",
            "  'name': 'serving_default_input_ids:0',\n",
            "  'quantization': (0.0, 0),\n",
            "  'quantization_parameters': {'quantized_dimension': 0,\n",
            "                              'scales': array([], dtype=float32),\n",
            "                              'zero_points': array([], dtype=int32)},\n",
            "  'shape': array([  1, 128], dtype=int32),\n",
            "  'shape_signature': array([-1, -1], dtype=int32),\n",
            "  'sparsity_parameters': {}}]\n",
            "TEXT: URGENT: Your account will be suspended. Click http://fake.example to verify\n",
            "pred: 1 probs: [2.038405364146456e-05, 0.9999796152114868]\n",
            "Resized input 'serving_default_attention_mask:0' from [np.int32(1), np.int32(1)] -> [1, 128]\n",
            "Resized input 'serving_default_input_ids:0' from [np.int32(1), np.int32(1)] -> [1, 128]\n",
            "Re-queried input details (after resize & allocate):\n",
            "[{'dtype': <class 'numpy.int32'>,\n",
            "  'index': 0,\n",
            "  'name': 'serving_default_attention_mask:0',\n",
            "  'quantization': (0.0, 0),\n",
            "  'quantization_parameters': {'quantized_dimension': 0,\n",
            "                              'scales': array([], dtype=float32),\n",
            "                              'zero_points': array([], dtype=int32)},\n",
            "  'shape': array([  1, 128], dtype=int32),\n",
            "  'shape_signature': array([-1, -1], dtype=int32),\n",
            "  'sparsity_parameters': {}},\n",
            " {'dtype': <class 'numpy.int32'>,\n",
            "  'index': 1,\n",
            "  'name': 'serving_default_input_ids:0',\n",
            "  'quantization': (0.0, 0),\n",
            "  'quantization_parameters': {'quantized_dimension': 0,\n",
            "                              'scales': array([], dtype=float32),\n",
            "                              'zero_points': array([], dtype=int32)},\n",
            "  'shape': array([  1, 128], dtype=int32),\n",
            "  'shape_signature': array([-1, -1], dtype=int32),\n",
            "  'sparsity_parameters': {}}]\n",
            "TEXT: Hey, let's meet tomorrow for lunch\n",
            "pred: 0 probs: [0.9999953508377075, 4.688928584073437e-06]\n",
            "\n",
            "Running smoke-test for: phishing_detector_int8.tflite\n",
            "input_details: [{'name': 'serving_default_attention_mask:0', 'index': 0, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([-1, -1], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'serving_default_input_ids:0', 'index': 1, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([-1, -1], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "output_details: [{'name': 'StatefulPartitionedCall:0', 'index': 718, 'shape': array([1, 2], dtype=int32), 'shape_signature': array([-1,  2], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.04853260517120361, -13), 'quantization_parameters': {'scales': array([0.04853261], dtype=float32), 'zero_points': array([-13], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "Resized input 'serving_default_attention_mask:0' from [np.int32(1), np.int32(1)] -> [1, 128]\n",
            "Resized input 'serving_default_input_ids:0' from [np.int32(1), np.int32(1)] -> [1, 128]\n",
            "Re-queried input details (after resize & allocate):\n",
            "[{'dtype': <class 'numpy.int32'>,\n",
            "  'index': 0,\n",
            "  'name': 'serving_default_attention_mask:0',\n",
            "  'quantization': (0.0, 0),\n",
            "  'quantization_parameters': {'quantized_dimension': 0,\n",
            "                              'scales': array([], dtype=float32),\n",
            "                              'zero_points': array([], dtype=int32)},\n",
            "  'shape': array([  1, 128], dtype=int32),\n",
            "  'shape_signature': array([-1, -1], dtype=int32),\n",
            "  'sparsity_parameters': {}},\n",
            " {'dtype': <class 'numpy.int32'>,\n",
            "  'index': 1,\n",
            "  'name': 'serving_default_input_ids:0',\n",
            "  'quantization': (0.0, 0),\n",
            "  'quantization_parameters': {'quantized_dimension': 0,\n",
            "                              'scales': array([], dtype=float32),\n",
            "                              'zero_points': array([], dtype=int32)},\n",
            "  'shape': array([  1, 128], dtype=int32),\n",
            "  'shape_signature': array([-1, -1], dtype=int32),\n",
            "  'sparsity_parameters': {}}]\n",
            "TEXT: URGENT: Your account will be suspended. Click http://fake.example to verify\n",
            "pred: 0 probs: [0.996906578540802, 0.003093418898060918]\n",
            "Resized input 'serving_default_attention_mask:0' from [np.int32(1), np.int32(1)] -> [1, 128]\n",
            "Resized input 'serving_default_input_ids:0' from [np.int32(1), np.int32(1)] -> [1, 128]\n",
            "Re-queried input details (after resize & allocate):\n",
            "[{'dtype': <class 'numpy.int32'>,\n",
            "  'index': 0,\n",
            "  'name': 'serving_default_attention_mask:0',\n",
            "  'quantization': (0.0, 0),\n",
            "  'quantization_parameters': {'quantized_dimension': 0,\n",
            "                              'scales': array([], dtype=float32),\n",
            "                              'zero_points': array([], dtype=int32)},\n",
            "  'shape': array([  1, 128], dtype=int32),\n",
            "  'shape_signature': array([-1, -1], dtype=int32),\n",
            "  'sparsity_parameters': {}},\n",
            " {'dtype': <class 'numpy.int32'>,\n",
            "  'index': 1,\n",
            "  'name': 'serving_default_input_ids:0',\n",
            "  'quantization': (0.0, 0),\n",
            "  'quantization_parameters': {'quantized_dimension': 0,\n",
            "                              'scales': array([], dtype=float32),\n",
            "                              'zero_points': array([], dtype=int32)},\n",
            "  'shape': array([  1, 128], dtype=int32),\n",
            "  'shape_signature': array([-1, -1], dtype=int32),\n",
            "  'sparsity_parameters': {}}]\n",
            "TEXT: Hey, let's meet tomorrow for lunch\n",
            "pred: 0 probs: [0.99998939037323, 1.0610091521812137e-05]\n"
          ]
        }
      ],
      "id": "WQXymn0rGnmo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8) Package artifacts and copy to Google Drive\n",
        "\n",
        "This cell packages the TFLite artifacts (float16/dynamic/int8), the tokenizer folder, runs a small evaluation on a sample of `test.csv` (if present), writes a README with sizes and quick metrics, zips the package, and copies it to Drive under `phishing_detector_artifacts_v2`.\n",
        "\n",
        "Run this cell after you have the `.tflite` files and the tokenizer saved to `./phishing_detector_model` or `./tokenizer`."
      ],
      "metadata": {
        "id": "FxExuVjdKLRK"
      },
      "id": "FxExuVjdKLRK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e109ccbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6757c6be-63dd-47fd-a77d-334665032827"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "Evaluating phishing_detector.tflite on a sample of up to 2000 rows...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating phishing_detector_dynamic.tflite on a sample of up to 2000 rows...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating phishing_detector_int8.tflite on a sample of up to 2000 rows...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n",
            "/tmp/ipython-input-2855770627.py:139: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  now = datetime.utcnow().isoformat() + 'Z'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Packaged artifacts -> /content/phishing_detector_package.zip\n",
            "Copied archive to Drive at /content/drive/MyDrive/phishing_detector_artifacts_v2\n",
            "Contents written to phishing_detector_package\n"
          ]
        }
      ],
      "source": [
        "# Packaging & export cell: create a package, run light evaluation, and copy to Drive\n",
        "import os, shutil, json, time, pathlib\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer\n",
        "try:\n",
        "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "    SKLEARN = True\n",
        "except Exception:\n",
        "    SKLEARN = False\n",
        "\n",
        "# Config\n",
        "DRIVE_DST = '/content/drive/MyDrive/phishing_detector_artifacts_v2'  # change if you want a different path\n",
        "LOCAL_PACKAGE = 'phishing_detector_package'\n",
        "EVAL_SAMPLE = 2000  # number of test rows to sample for quick evaluation (set lower if you want faster runs)\n",
        "MAX_LEN = 128\n",
        "\n",
        "# Mount Drive (will prompt if not mounted yet)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "os.makedirs(DRIVE_DST, exist_ok=True)\n",
        "shutil.rmtree(LOCAL_PACKAGE, ignore_errors=True)\n",
        "os.makedirs(LOCAL_PACKAGE, exist_ok=True)\n",
        "\n",
        "# Gather available TFLite artifacts\n",
        "candidates = [\n",
        "    ('float16', 'phishing_detector.tflite'),\n",
        "    ('dynamic', 'phishing_detector_dynamic.tflite'),\n",
        "    ('int8', 'phishing_detector_int8.tflite'),\n",
        "]\n",
        "found = []\n",
        "for qtype, fname in candidates:\n",
        "    if os.path.exists(fname):\n",
        "        size_mb = round(os.path.getsize(fname) / (1024*1024), 2)\n",
        "        shutil.copy(fname, os.path.join(LOCAL_PACKAGE, fname))\n",
        "        found.append({'quant': qtype, 'file': fname, 'size_mb': size_mb})\n",
        "\n",
        "# Copy tokenizer / tokenizer directory\n",
        "tokenizer_src = None\n",
        "for tok_dir in ('tokenizer', 'phishing_detector_model', './phishing_detector_model'):\n",
        "    if os.path.exists(tok_dir) and os.path.isdir(tok_dir):\n",
        "        tokenizer_src = tok_dir\n",
        "        shutil.copytree(tok_dir, os.path.join(LOCAL_PACKAGE, 'tokenizer'), dirs_exist_ok=True)\n",
        "        break\n",
        "if tokenizer_src is None:\n",
        "    print('Warning: tokenizer folder not found; ensure you include tokenizer files when packaging')\n",
        "\n",
        "# Light evaluation helper (works on a sample of test set to keep runtime short)\n",
        "\n",
        "def run_tflite_eval(tflite_path, test_texts, test_labels, max_eval=500):\n",
        "    # Build interpreter and resize inputs to (1, MAX_LEN)\n",
        "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    # Resize inputs to (1, MAX_LEN)\n",
        "    for inp in input_details:\n",
        "        interpreter.resize_tensor_input(inp['index'], [1, MAX_LEN])\n",
        "    interpreter.allocate_tensors()\n",
        "    input_details = interpreter.get_input_details()\n",
        "    # Evaluate on up to max_eval examples\n",
        "    preds = []\n",
        "    for text in test_texts[:max_eval]:\n",
        "        enc = tokenizer(text, truncation=True, padding='max_length', max_length=MAX_LEN, return_tensors='np')\n",
        "        enc = {k: np.asarray(v) for k,v in enc.items()}\n",
        "        # set inputs in interpreter order\n",
        "        for inp in input_details:\n",
        "            name = inp['name'].lower()\n",
        "            if 'input_ids' in name:\n",
        "                arr = enc.get('input_ids')\n",
        "            elif 'attention_mask' in name:\n",
        "                arr = enc.get('attention_mask')\n",
        "            else:\n",
        "                arr = enc.get('input_ids')\n",
        "            # cast if interpreter expects int8\n",
        "            if inp['dtype'] == np.int8:\n",
        "                arr = arr.astype(np.int8)\n",
        "            else:\n",
        "                arr = arr.astype(inp['dtype'])\n",
        "            interpreter.set_tensor(inp['index'], arr)\n",
        "        interpreter.invoke()\n",
        "        out = interpreter.get_tensor(output_details[0]['index'])\n",
        "        # dequantize if int8 output\n",
        "        if output_details[0]['dtype'] == np.int8:\n",
        "            scale, zero_point = output_details[0]['quantization']\n",
        "            out = (out.astype(np.float32) - zero_point) * scale\n",
        "        # compute pred\n",
        "        if out.ndim == 2 and out.shape[1] >= 2:\n",
        "            pred = int(np.argmax(out, axis=-1)[0])\n",
        "        else:\n",
        "            # fallback: sigmoid scenario\n",
        "            s = 1.0/(1.0+np.exp(-out.ravel()[0]))\n",
        "            pred = int(s > 0.5)\n",
        "        preds.append(pred)\n",
        "    # metrics\n",
        "    if len(preds) == 0:\n",
        "        return None\n",
        "    if SKLEARN:\n",
        "        acc = accuracy_score(test_labels[:min(len(test_labels), max_eval)], preds)\n",
        "        p, r, f1, _ = precision_recall_fscore_support(test_labels[:min(len(test_labels), max_eval)], preds, average='binary')\n",
        "        return {'accuracy': float(acc), 'precision': float(p), 'recall': float(r), 'f1': float(f1), 'n': min(len(test_labels), max_eval)}\n",
        "    else:\n",
        "        # simple accuracy fallback\n",
        "        true = np.array(test_labels[:min(len(test_labels), max_eval)])\n",
        "        arrp = np.array(preds)\n",
        "        acc = float((arrp == true).mean())\n",
        "        return {'accuracy': acc, 'n': min(len(test_labels), max_eval)}\n",
        "\n",
        "# Prepare test data (sample) if available\n",
        "tokenizer = AutoTokenizer.from_pretrained('./phishing_detector_model') if tokenizer_src else None\n",
        "test_df = None\n",
        "if os.path.exists('test.csv'):\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "elif os.path.exists('data/processed/test.csv'):\n",
        "    test_df = pd.read_csv('data/processed/test.csv')\n",
        "\n",
        "eval_results = {}\n",
        "if test_df is not None and 'message' in test_df.columns and 'label' in test_df.columns:\n",
        "    # ensure numeric labels 0/1\n",
        "    if test_df['label'].dtype != 'int64' and test_df['label'].dtype != 'int32':\n",
        "        # try map strings to ints\n",
        "        test_df['label'] = test_df['label'].map({'phishing':1, 'legitimate':0}).fillna(test_df['label'])\n",
        "    labels = test_df['label'].astype(int).tolist()\n",
        "    texts = test_df['message'].astype(str).tolist()\n",
        "    # Evaluate each found tflite file (only short sample to keep time reasonable)\n",
        "    for meta in found:\n",
        "        q = meta['quant']\n",
        "        fname = meta['file']\n",
        "        local_path = os.path.join(LOCAL_PACKAGE, fname)\n",
        "        print('\\nEvaluating', fname, 'on a sample of up to', EVAL_SAMPLE, 'rows...')\n",
        "        res = run_tflite_eval(local_path, texts, labels, max_eval=min(EVAL_SAMPLE, len(texts)))\n",
        "        eval_results[fname] = res\n",
        "else:\n",
        "    print('No test.csv found for evaluation; skipping evaluation step')\n",
        "\n",
        "# Create README with metadata and evaluation results\n",
        "readme_path = os.path.join(LOCAL_PACKAGE, 'README.md')\n",
        "now = datetime.utcnow().isoformat() + 'Z'\n",
        "with open(readme_path, 'w') as fh:\n",
        "    fh.write('# Phishing detector artifacts\\n')\n",
        "    fh.write('\\nCreated: {}\\n'.format(now))\n",
        "    fh.write('\\nModel: distilbert-base-multilingual-cased\\n')\n",
        "    fh.write('max_length: {}\\n'.format(MAX_LEN))\n",
        "    fh.write('input_names: serving_default_input_ids:0, serving_default_attention_mask:0\\n')\n",
        "    fh.write('\\nFiles included:\\n')\n",
        "    for m in found:\n",
        "        fh.write('- {file}  ({quant}, {size_mb} MB)\\n'.format(**m))\n",
        "    if tokenizer_src:\n",
        "        fh.write('- tokenizer folder: {}\\n'.format(tokenizer_src))\n",
        "    fh.write('\\nEvaluation results (sample):\\n')\n",
        "    fh.write(json.dumps(eval_results, indent=2))\n",
        "\n",
        "# Save a small CSV of sample predictions (optional) -- here we'll skip for brevity unless user wants it explicitly\n",
        "\n",
        "# Zip package and copy to Drive\n",
        "archive_name = shutil.make_archive(LOCAL_PACKAGE, 'zip', LOCAL_PACKAGE)\n",
        "shutil.copy(archive_name, DRIVE_DST)\n",
        "print('Packaged artifacts ->', archive_name)\n",
        "print('Copied archive to Drive at', DRIVE_DST)\n",
        "print('Contents written to', LOCAL_PACKAGE)\n"
      ],
      "id": "e109ccbb"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "!ls -lah /content/drive/MyDrive/phishing_detector_artifacts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-Z3Rm2PYLKE",
        "outputId": "54ca2c74-f3f7-4cc6-8a14-c6ac94360616"
      },
      "id": "W-Z3Rm2PYLKE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "total 1.2G\n",
            "drwx------ 2 root root 4.0K Oct 23 23:24 mobile\n",
            "drwx------ 2 root root 4.0K Oct 22 17:33 phishing_detector_model\n",
            "-rw------- 1 root root 908M Oct 23 23:27 phishing_detector_package.zip\n",
            "-rw------- 1 root root 259M Oct 22 17:33 phishing_detector.tflite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip into /content so scripts can access the files quickly\n",
        "!mkdir -p /content/phishing_detector_package\n",
        "!unzip -o \"/content/drive/MyDrive/phishing_detector_artifacts/phishing_detector_package.zip\" -d /content/phishing_detector_package || true\n",
        "\n",
        "# list the files we need\n",
        "!ls -lah /content/phishing_detector_package\n",
        "!ls -lah /content/phishing_detector_package/tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfNKKbJVcJN3",
        "outputId": "b7e4eeaf-0397-41dd-9b01-c10e80697170"
      },
      "id": "mfNKKbJVcJN3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/phishing_detector_artifacts/phishing_detector_package.zip\n",
            "  inflating: /content/phishing_detector_package/phishing_detector_int8.tflite  \n",
            "  inflating: /content/phishing_detector_package/phishing_detector_dynamic.tflite  \n",
            "  inflating: /content/phishing_detector_package/phishing_detector.tflite  \n",
            "  inflating: /content/phishing_detector_package/README.md  \n",
            "  inflating: /content/phishing_detector_package/tokenizer/special_tokens_map.json  \n",
            "  inflating: /content/phishing_detector_package/tokenizer/vocab.txt  \n",
            "  inflating: /content/phishing_detector_package/tokenizer/model.safetensors  \n",
            "  inflating: /content/phishing_detector_package/tokenizer/tokenizer.json  \n",
            "  inflating: /content/phishing_detector_package/tokenizer/tokenizer_config.json  \n",
            "  inflating: /content/phishing_detector_package/tokenizer/training_args.bin  \n",
            "  inflating: /content/phishing_detector_package/tokenizer/config.json  \n",
            "total 518M\n",
            "drwxr-xr-x 3 root root 4.0K Oct 24 01:20 .\n",
            "drwxr-xr-x 1 root root 4.0K Oct 24 01:05 ..\n",
            "-rw-r--r-- 1 root root  16K Oct 24 01:15 mock_parity.jsonl\n",
            "-rw-r--r-- 1 root root  657 Oct 24 01:04 model-metadata.json\n",
            "-rw-r--r-- 1 root root 130M Oct 23 17:43 phishing_detector_dynamic.tflite\n",
            "-rw-r--r-- 1 root root 130M Oct 23 17:43 phishing_detector_int8.tflite\n",
            "-rw-r--r-- 1 root root 259M Oct 23 17:43 phishing_detector.tflite\n",
            "-rw-r--r-- 1 root root  993 Oct 23 18:04 README.md\n",
            "drwxr-xr-x 2 root root 4.0K Oct 24 01:20 tokenizer\n",
            "total 521M\n",
            "drwxr-xr-x 2 root root 4.0K Oct 24 01:20 .\n",
            "drwxr-xr-x 3 root root 4.0K Oct 24 01:20 ..\n",
            "-rw-r--r-- 1 root root  587 Oct 23 16:13 config.json\n",
            "-rw-r--r-- 1 root root 517M Oct 23 16:13 model.safetensors\n",
            "-rw-r--r-- 1 root root  125 Oct 23 16:13 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 1.2K Oct 23 16:13 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root 2.8M Oct 23 16:13 tokenizer.json\n",
            "-rw-r--r-- 1 root root 5.8K Oct 23 16:13 training_args.bin\n",
            "-rw-r--r-- 1 root root 973K Oct 23 16:13 vocab.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Option A: copy the mobile folder into /content (faster IO)\n",
        "!cp -r \"/content/drive/MyDrive/phishing_detector_artifacts/mobile\" /content/mobile || true\n",
        "!ls -lah /content/mobile/inference_wrapper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qlq-pwhKcXl4",
        "outputId": "037a8242-8eae-4a88-c82e-f9d25a568613"
      },
      "id": "Qlq-pwhKcXl4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 40K\n",
            "drwx------ 4 root root 4.0K Oct 24 01:03 .\n",
            "drwx------ 3 root root 4.0K Oct 24 01:03 ..\n",
            "-rw------- 1 root root 8.7K Oct 24 01:03 infer.py\n",
            "drwx------ 2 root root 4.0K Oct 24 01:03 __pycache__\n",
            "-rw------- 1 root root 1.5K Oct 24 01:03 README.md\n",
            "-rw------- 1 root root  147 Oct 24 01:03 requirements.txt\n",
            "drwx------ 2 root root 4.0K Oct 24 01:03 schema\n",
            "-rw------- 1 root root 1.7K Oct 24 01:03 smoke_run.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUQVNu0mc2Gn"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers tokenizers pandas jsonschema requests"
      ],
      "id": "iUQVNu0mc2Gn"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow:\", tf.__version__)\n",
        "# quick TFLite interpreter sanity check (don't load the large model yet)\n",
        "from tensorflow.lite.python.interpreter import Interpreter\n",
        "print(\"Interpreter OK\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baDguklYdWJX",
        "outputId": "317ed57e-de29-4d56-ba57-3ff80e5b090b"
      },
      "id": "baDguklYdWJX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow: 2.19.0\n",
            "Interpreter OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()   # choose files in the browser dialog\n",
        "# move file into the repo folder structure expected by the script:\n",
        "import os\n",
        "os.makedirs('/content/data/processed', exist_ok=True)\n",
        "for fname in uploaded.keys():\n",
        "    # for example if you uploaded test.csv\n",
        "    if fname.endswith('.csv'):\n",
        "        os.replace(fname, f'/content/data/processed/{fname}')\n",
        "print('Files moved to /content/data/processed/')\n",
        "!ls -lah /content/data/processed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "n9fy3g3seixY",
        "outputId": "b674370e-544e-4604-eb84-38e53ab85e11"
      },
      "id": "n9fy3g3seixY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c5ef322b-c8c8-4a46-b138-2169c9e7b2b6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c5ef322b-c8c8-4a46-b138-2169c9e7b2b6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test.csv to test.csv\n",
            "Files moved to /content/data/processed/\n",
            "total 1.9M\n",
            "drwxr-xr-x 2 root root 4.0K Oct 24 01:05 .\n",
            "drwxr-xr-x 3 root root 4.0K Oct 24 01:05 ..\n",
            "-rw-r--r-- 1 root root 1.9M Oct 24 01:05 test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run a short smoke test (10 messages) to validate end-to-end\n",
        "!python /content/mobile/inference_wrapper/smoke_run.py -n 20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KB32uYrdaxP",
        "outputId": "e4aeaca4-42e8-4e55-9545-d8f5f414b106"
      },
      "id": "9KB32uYrdaxP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-10-24 00:14:26.141804: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761264866.161773   15667 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761264866.167782   15667 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761264866.183243   15667 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761264866.183281   15667 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761264866.183285   15667 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761264866.183288   15667 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "Wrote 20 detections to /content/phishing_detector_package/sample_detections.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python mobile/inference_wrapper/run_on_mock.py --weights 0.825,0.85,0.875 --out ./sweep_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXtcTWXovYnh",
        "outputId": "5b43d64a-7da4-480e-dcb2-4a079073b398"
      },
      "id": "DXtcTWXovYnh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-10-24 02:34:28.135544: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761273268.193475   23454 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761273268.217225   23454 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761273268.254642   23454 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761273268.254701   23454 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761273268.254711   23454 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761273268.254720   23454 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n",
            "\n",
            "Running sweep with heuristic_weight=0.825\n",
            "\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "Message mock-1 body: UBA Alert: Your account will be suspended within 24 hours. Verify your account now at http://uba-secure-check.com to avoid blockage.\n",
            "\n",
            "  mock_score=0.7  wrapper_score=0.99  delta=0.29\n",
            "  raw_model_score=1.0  heuristic_score=1.0  combined_pre_clamp=1.0\n",
            "  mock matches: ['Urgency language', 'Account verification request']\n",
            "  wrapper factors/matches: ['Urgency language', 'Link', 'Financial keyword', 'Account action requested']\n",
            "\n",
            "Message mock-2 body: Congratulations! You qualify for a special tax rebate. Tap this link to claim within 12 hours: https://bit.ly/rebatesafrica\n",
            "\n",
            "  mock_score=0.45  wrapper_score=0.53  delta=0.08\n",
            "  raw_model_score=1.0  heuristic_score=0.43  combined_pre_clamp=0.5297\n",
            "  mock matches: ['Link-based call-to-action']\n",
            "  wrapper factors/matches: ['Suspicious link', 'Reward / lottery']\n",
            "\n",
            "Message mock-3 body: Dear customer, your SIM will be deactivated today. Confirm your NIN immediately via http://mtn-verify.ng and enter your OTP.\n",
            "\n",
            "  mock_score=0.65  wrapper_score=0.99  delta=0.34\n",
            "  raw_model_score=0.9994  heuristic_score=1.0  combined_pre_clamp=0.9999\n",
            "  mock matches: ['Urgency language', 'Credential or OTP request']\n",
            "  wrapper factors/matches: ['Urgency language', 'Link', 'Account action requested']\n",
            "\n",
            "Message mock-4 body: We need you to update your payroll information before salaries are processed. Click the link and input your banking PIN to continue.\n",
            "\n",
            "  mock_score=0.6  wrapper_score=0.78  delta=0.18\n",
            "  raw_model_score=0.999  heuristic_score=0.73  combined_pre_clamp=0.7771\n",
            "  mock matches: ['Link-based call-to-action', 'Financial institution reference', 'Credential or OTP request']\n",
            "  wrapper factors/matches: ['Urgency language', 'Financial keyword', 'Account action requested']\n",
            "\n",
            "Message mock-5 body: You have been selected for an Airtel Rewards gift. Act now and claim your prize code: http://airtel-bonus.win\n",
            "\n",
            "  mock_score=0.68  wrapper_score=0.74  delta=0.06\n",
            "  raw_model_score=0.9982  heuristic_score=0.69  combined_pre_clamp=0.7439\n",
            "  mock matches: ['Urgency language', 'Unexpected reward']\n",
            "  wrapper factors/matches: ['Urgency language', 'Link', 'Reward / lottery']\n",
            "\n",
            "Message mock-6 body: WhatsApp: Your chats will be deleted. Confirm your account using the OTP sent to you. Failure to respond means suspension.\n",
            "\n",
            "  mock_score=0.55  wrapper_score=0.5  delta=-0.05\n",
            "  raw_model_score=1.0  heuristic_score=0.4  combined_pre_clamp=0.505\n",
            "  mock matches: ['Account verification request', 'Credential or OTP request']\n",
            "  wrapper factors/matches: ['Financial keyword', 'Account action requested']\n",
            "\n",
            "Message mock-7 body: Your Stanbic account has been flagged. Update your BVN immediately using this secure portal: http://stanbic-review.info\n",
            "\n",
            "  mock_score=0.5  wrapper_score=0.93  delta=0.43\n",
            "  raw_model_score=0.999  heuristic_score=0.91  combined_pre_clamp=0.9256\n",
            "  mock matches: ['Urgency language']\n",
            "  wrapper factors/matches: ['Urgency language', 'Link', 'Financial keyword', 'Account action requested']\n",
            "\n",
            "Message benign-1 body: Hey, are we still on for dinner tonight at 7?\n",
            "\n",
            "  mock_score=0.25  wrapper_score=0.0  delta=-0.25\n",
            "  raw_model_score=0.0  heuristic_score=0.0  combined_pre_clamp=0.0\n",
            "  mock matches: []\n",
            "  wrapper factors/matches: []\n",
            "\n",
            "Message benign-2 body: Your order #12345 has shipped. Track at https://shopxyz.com/track/12345\n",
            "\n",
            "  mock_score=0.15  wrapper_score=0.42  delta=0.27\n",
            "  raw_model_score=1.0  heuristic_score=0.3  combined_pre_clamp=0.4225\n",
            "  mock matches: []\n",
            "  wrapper factors/matches: ['Link', 'OTP/Verification code']\n",
            "\n",
            "Message benign-3 body: Your account ending 4321 was credited with NGN 5,000.00 on 2025-10-08.\n",
            "\n",
            "  mock_score=0.25  wrapper_score=0.44  delta=0.19\n",
            "  raw_model_score=1.0  heuristic_score=0.32  combined_pre_clamp=0.439\n",
            "  mock matches: []\n",
            "  wrapper factors/matches: ['OTP/Verification code', 'Financial keyword']\n",
            "\n",
            "Message benign-4 body: Your package is out for delivery and will arrive today between 2-5pm.\n",
            "\n",
            "  mock_score=0.2  wrapper_score=0.17  delta=-0.03\n",
            "  raw_model_score=0.9902  heuristic_score=0.0  combined_pre_clamp=0.1733\n",
            "  mock matches: []\n",
            "  wrapper factors/matches: []\n",
            "\n",
            "Message benign-5 body: Monthly newsletter: tips to keep your account secure.\n",
            "\n",
            "  mock_score=0.15  wrapper_score=0.34  delta=0.19\n",
            "  raw_model_score=0.9996  heuristic_score=0.2  combined_pre_clamp=0.3399\n",
            "  mock matches: []\n",
            "  wrapper factors/matches: ['Financial keyword']\n",
            "\n",
            "Wrote per-weight parity results to ./sweep_results_w0_825\n",
            "\n",
            "Running sweep with heuristic_weight=0.85\n",
            "\n",
            "Message mock-1 body: UBA Alert: Your account will be suspended within 24 hours. Verify your account now at http://uba-secure-check.com to avoid blockage.\n",
            "\n",
            "  mock_score=0.7  wrapper_score=0.99  delta=0.29\n",
            "  raw_model_score=1.0  heuristic_score=1.0  combined_pre_clamp=1.0\n",
            "  mock matches: ['Urgency language', 'Account verification request']\n",
            "  wrapper factors/matches: ['Urgency language', 'Link', 'Financial keyword', 'Account action requested']\n",
            "\n",
            "Message mock-2 body: Congratulations! You qualify for a special tax rebate. Tap this link to claim within 12 hours: https://bit.ly/rebatesafrica\n",
            "\n",
            "  mock_score=0.45  wrapper_score=0.52  delta=0.07\n",
            "  raw_model_score=1.0  heuristic_score=0.43  combined_pre_clamp=0.5155\n",
            "  mock matches: ['Link-based call-to-action']\n",
            "  wrapper factors/matches: ['Suspicious link', 'Reward / lottery']\n",
            "\n",
            "Message mock-3 body: Dear customer, your SIM will be deactivated today. Confirm your NIN immediately via http://mtn-verify.ng and enter your OTP.\n",
            "\n",
            "  mock_score=0.65  wrapper_score=0.99  delta=0.34\n",
            "  raw_model_score=0.9994  heuristic_score=1.0  combined_pre_clamp=0.9999\n",
            "  mock matches: ['Urgency language', 'Credential or OTP request']\n",
            "  wrapper factors/matches: ['Urgency language', 'Link', 'Account action requested']\n",
            "\n",
            "Message mock-4 body: We need you to update your payroll information before salaries are processed. Click the link and input your banking PIN to continue.\n",
            "\n",
            "  mock_score=0.6  wrapper_score=0.77  delta=0.17\n",
            "  raw_model_score=0.999  heuristic_score=0.73  combined_pre_clamp=0.7704\n",
            "  mock matches: ['Link-based call-to-action', 'Financial institution reference', 'Credential or OTP request']\n",
            "  wrapper factors/matches: ['Urgency language', 'Financial keyword', 'Account action requested']\n",
            "\n",
            "Message mock-5 body: You have been selected for an Airtel Rewards gift. Act now and claim your prize code: http://airtel-bonus.win\n",
            "\n",
            "  mock_score=0.68  wrapper_score=0.74  delta=0.06\n",
            "  raw_model_score=0.9982  heuristic_score=0.69  combined_pre_clamp=0.7362\n",
            "  mock matches: ['Urgency language', 'Unexpected reward']\n",
            "  wrapper factors/matches: ['Urgency language', 'Link', 'Reward / lottery']\n",
            "\n",
            "Message mock-6 body: WhatsApp: Your chats will be deleted. Confirm your account using the OTP sent to you. Failure to respond means suspension.\n",
            "\n",
            "  mock_score=0.55  wrapper_score=0.49  delta=-0.06\n",
            "  raw_model_score=1.0  heuristic_score=0.4  combined_pre_clamp=0.49\n",
            "  mock matches: ['Account verification request', 'Credential or OTP request']\n",
            "  wrapper factors/matches: ['Financial keyword', 'Account action requested']\n",
            "\n",
            "Message mock-7 body: Your Stanbic account has been flagged. Update your BVN immediately using this secure portal: http://stanbic-review.info\n",
            "\n",
            "  mock_score=0.5  wrapper_score=0.92  delta=0.42\n",
            "  raw_model_score=0.999  heuristic_score=0.91  combined_pre_clamp=0.9234\n",
            "  mock matches: ['Urgency language']\n",
            "  wrapper factors/matches: ['Urgency language', 'Link', 'Financial keyword', 'Account action requested']\n",
            "\n",
            "Message benign-1 body: Hey, are we still on for dinner tonight at 7?\n",
            "\n",
            "  mock_score=0.25  wrapper_score=0.0  delta=-0.25\n",
            "  raw_model_score=0.0  heuristic_score=0.0  combined_pre_clamp=0.0\n",
            "  mock matches: []\n",
            "  wrapper factors/matches: []\n",
            "\n",
            "Message benign-2 body: Your order #12345 has shipped. Track at https://shopxyz.com/track/12345\n",
            "\n",
            "  mock_score=0.15  wrapper_score=0.4  delta=0.25\n",
            "  raw_model_score=1.0  heuristic_score=0.3  combined_pre_clamp=0.405\n",
            "  mock matches: []\n",
            "  wrapper factors/matches: ['Link', 'OTP/Verification code']\n",
            "\n",
            "Message benign-3 body: Your account ending 4321 was credited with NGN 5,000.00 on 2025-10-08.\n",
            "\n",
            "  mock_score=0.25  wrapper_score=0.42  delta=0.17\n",
            "  raw_model_score=1.0  heuristic_score=0.32  combined_pre_clamp=0.422\n",
            "  mock matches: []\n",
            "  wrapper factors/matches: ['OTP/Verification code', 'Financial keyword']\n",
            "\n",
            "Message benign-4 body: Your package is out for delivery and will arrive today between 2-5pm.\n",
            "\n",
            "  mock_score=0.2  wrapper_score=0.15  delta=-0.05\n",
            "  raw_model_score=0.9902  heuristic_score=0.0  combined_pre_clamp=0.1485\n",
            "  mock matches: []\n",
            "  wrapper factors/matches: []\n",
            "\n",
            "Message benign-5 body: Monthly newsletter: tips to keep your account secure.\n",
            "\n",
            "  mock_score=0.15  wrapper_score=0.32  delta=0.17\n",
            "  raw_model_score=0.9996  heuristic_score=0.2  combined_pre_clamp=0.3199\n",
            "  mock matches: []\n",
            "  wrapper factors/matches: ['Financial keyword']\n",
            "\n",
            "Wrote per-weight parity results to ./sweep_results_w0_85\n",
            "\n",
            "Running sweep with heuristic_weight=0.875\n",
            "\n",
            "Message mock-1 body: UBA Alert: Your account will be suspended within 24 hours. Verify your account now at http://uba-secure-check.com to avoid blockage.\n",
            "\n",
            "  mock_score=0.7  wrapper_score=0.99  delta=0.29\n",
            "  raw_model_score=1.0  heuristic_score=1.0  combined_pre_clamp=1.0\n",
            "  mock matches: ['Urgency language', 'Account verification request']\n",
            "  wrapper factors/matches: ['Urgency language', 'Link', 'Financial keyword', 'Account action requested']\n",
            "\n",
            "Message mock-2 body: Congratulations! You qualify for a special tax rebate. Tap this link to claim within 12 hours: https://bit.ly/rebatesafrica\n",
            "\n",
            "  mock_score=0.45  wrapper_score=0.5  delta=0.05\n",
            "  raw_model_score=1.0  heuristic_score=0.43  combined_pre_clamp=0.5012\n",
            "  mock matches: ['Link-based call-to-action']\n",
            "  wrapper factors/matches: ['Suspicious link', 'Reward / lottery']\n",
            "\n",
            "Message mock-3 body: Dear customer, your SIM will be deactivated today. Confirm your NIN immediately via http://mtn-verify.ng and enter your OTP.\n",
            "\n",
            "  mock_score=0.65  wrapper_score=0.99  delta=0.34\n",
            "  raw_model_score=0.9994  heuristic_score=1.0  combined_pre_clamp=0.9999\n",
            "  mock matches: ['Urgency language', 'Credential or OTP request']\n",
            "  wrapper factors/matches: ['Urgency language', 'Link', 'Account action requested']\n",
            "\n",
            "Message mock-4 body: We need you to update your payroll information before salaries are processed. Click the link and input your banking PIN to continue.\n",
            "\n",
            "  mock_score=0.6  wrapper_score=0.76  delta=0.16\n",
            "  raw_model_score=0.999  heuristic_score=0.73  combined_pre_clamp=0.7636\n",
            "  mock matches: ['Link-based call-to-action', 'Financial institution reference', 'Credential or OTP request']\n",
            "  wrapper factors/matches: ['Urgency language', 'Financial keyword', 'Account action requested']\n",
            "\n",
            "Message mock-5 body: You have been selected for an Airtel Rewards gift. Act now and claim your prize code: http://airtel-bonus.win\n",
            "\n",
            "  mock_score=0.68  wrapper_score=0.73  delta=0.05\n",
            "  raw_model_score=0.9982  heuristic_score=0.69  combined_pre_clamp=0.7285\n",
            "  mock matches: ['Urgency language', 'Unexpected reward']\n",
            "  wrapper factors/matches: ['Urgency language', 'Link', 'Reward / lottery']\n",
            "\n",
            "Message mock-6 body: WhatsApp: Your chats will be deleted. Confirm your account using the OTP sent to you. Failure to respond means suspension.\n",
            "\n",
            "  mock_score=0.55  wrapper_score=0.47  delta=-0.08\n",
            "  raw_model_score=1.0  heuristic_score=0.4  combined_pre_clamp=0.475\n",
            "  mock matches: ['Account verification request', 'Credential or OTP request']\n",
            "  wrapper factors/matches: ['Financial keyword', 'Account action requested']\n",
            "\n",
            "Message mock-7 body: Your Stanbic account has been flagged. Update your BVN immediately using this secure portal: http://stanbic-review.info\n",
            "\n",
            "  mock_score=0.5  wrapper_score=0.92  delta=0.42\n",
            "  raw_model_score=0.999  heuristic_score=0.91  combined_pre_clamp=0.9211\n",
            "  mock matches: ['Urgency language']\n",
            "  wrapper factors/matches: ['Urgency language', 'Link', 'Financial keyword', 'Account action requested']\n",
            "\n",
            "Message benign-1 body: Hey, are we still on for dinner tonight at 7?\n",
            "\n",
            "  mock_score=0.25  wrapper_score=0.0  delta=-0.25\n",
            "  raw_model_score=0.0  heuristic_score=0.0  combined_pre_clamp=0.0\n",
            "  mock matches: []\n",
            "  wrapper factors/matches: []\n",
            "\n",
            "Message benign-2 body: Your order #12345 has shipped. Track at https://shopxyz.com/track/12345\n",
            "\n",
            "  mock_score=0.15  wrapper_score=0.39  delta=0.24\n",
            "  raw_model_score=1.0  heuristic_score=0.3  combined_pre_clamp=0.3875\n",
            "  mock matches: []\n",
            "  wrapper factors/matches: ['Link', 'OTP/Verification code']\n",
            "\n",
            "Message benign-3 body: Your account ending 4321 was credited with NGN 5,000.00 on 2025-10-08.\n",
            "\n",
            "  mock_score=0.25  wrapper_score=0.4  delta=0.15\n",
            "  raw_model_score=1.0  heuristic_score=0.32  combined_pre_clamp=0.405\n",
            "  mock matches: []\n",
            "  wrapper factors/matches: ['OTP/Verification code', 'Financial keyword']\n",
            "\n",
            "Message benign-4 body: Your package is out for delivery and will arrive today between 2-5pm.\n",
            "\n",
            "  mock_score=0.2  wrapper_score=0.12  delta=-0.08\n",
            "  raw_model_score=0.9902  heuristic_score=0.0  combined_pre_clamp=0.1238\n",
            "  mock matches: []\n",
            "  wrapper factors/matches: []\n",
            "\n",
            "Message benign-5 body: Monthly newsletter: tips to keep your account secure.\n",
            "\n",
            "  mock_score=0.15  wrapper_score=0.3  delta=0.15\n",
            "  raw_model_score=0.9996  heuristic_score=0.2  combined_pre_clamp=0.2999\n",
            "  mock matches: []\n",
            "  wrapper factors/matches: ['Financial keyword']\n",
            "\n",
            "Wrote per-weight parity results to ./sweep_results_w0_875\n",
            "Wrote sweep summary to ./sweep_results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SwSC9r6evZe5"
      },
      "id": "SwSC9r6evZe5",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}